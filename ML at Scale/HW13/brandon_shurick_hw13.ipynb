{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW 13.4: Criteo Phase 2 baseline\n",
    "\n",
    "\n",
    ">SPECIAL NOTE:\n",
    "Please share your findings as they become available with class via the Google Group. You will get brownie points for this.  Once results are shared please use them and build on them.\n",
    "\n",
    ">The Criteo data for this challenge is located in the following S3/Dropbox buckets:\n",
    "\n",
    ">On Dropbox see:\n",
    "     https://www.dropbox.com/sh/dnevke9vsk6yj3p/AABoP-Kv2SRxuK8j3TtJsSv5a?dl=0\n",
    "\n",
    ">Raw Data:  (Training, Validation and Test data)\n",
    "https://console.aws.amazon.com/s3/home?region=us-west-1#&bucket=criteo-dataset&prefix=rawdata/\n",
    "\n",
    ">Hashed Data: Training, Validation and Test data in hash encoded (10,000 buckets) and sparse representation\n",
    "https://console.aws.amazon.com/s3/home?region=us-west-1#&bucket=criteo-dataset&prefix=processeddata/\n",
    "\n",
    "\n",
    ">Using the training dataset, validation dataset and testing dataset in the Criteo bucket perform the following experiment:\n",
    "\n",
    ">-- write spark code (borrow from Phase 1 of this project) to train a logistic regression model with the following hyperparamters:\n",
    "\n",
    ">-- Number of buckets for hashing: 1,000\n",
    "-- Logistic Regression: no regularization term\n",
    "-- Logistic Regression: step size = 10\n",
    "\n",
    ">Report the AWS cluster configuration that you used and how long in minutes and seconds it takes to complete this job.\n",
    "\n",
    ">Report in tabular form the AUC value (https://en.wikipedia.org/wiki/Receiver_operating_characteristic) for the Training, Validation, and Testing datasets.\n",
    "Report in tabular form  the logLossTest for the Training, Validation, and Testing datasets.\n",
    "\n",
    ">Dont forget to put a caption on your tables (above each table).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# spark functions \n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.linalg import SparseVector\n",
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
    "\n",
    "# helper libraries \n",
    "from collections import defaultdict\n",
    "import hashlib\n",
    "import numpy as np\n",
    "import datetime as DT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def oneHotEncoding(rawFeats, OHEDict, numOHEFeats):\n",
    "    \"\"\"Produce a one-hot-encoding from a list of features and an OHE dictionary.\n",
    "\n",
    "    Note:\n",
    "        If a (featureID, value) tuple doesn't have a corresponding key in OHEDict it should be\n",
    "        ignored.\n",
    "\n",
    "    Args:\n",
    "        rawFeats (list of (int, str)): The features corresponding to a single observation.  Each\n",
    "            feature consists of a tuple of featureID and the feature's value. (e.g. sampleOne)\n",
    "        OHEDict (dict): A mapping of (featureID, value) to unique integer.\n",
    "        numOHEFeats (int): The total number of unique OHE features (combinations of featureID and\n",
    "            value).\n",
    "\n",
    "    Returns:\n",
    "        SparseVector: A SparseVector of length numOHEFeats with indicies equal to the unique\n",
    "            identifiers for the (featureID, value) combinations that occur in the observation and\n",
    "            with values equal to 1.0.\n",
    "    \"\"\"\n",
    "    ref = {}\n",
    "    for k in rawFeats:\n",
    "        if k in OHEDict:\n",
    "            ref.update({OHEDict[k]:1})\n",
    "    return SparseVector(numOHEFeats, ref)\n",
    "\n",
    "def parseOHEPoint(point, OHEDict, numOHEFeats, delim):\n",
    "    \"\"\"Obtain the label and feature vector for this raw observation.\n",
    "\n",
    "    Note:\n",
    "        You must use the function `oneHotEncoding` in this implementation or later portions\n",
    "        of this lab may not function as expected.\n",
    "\n",
    "    Args:\n",
    "        point (str): A comma separated string where the first value is the label and the rest\n",
    "            are features.\n",
    "        OHEDict (dict of (int, str) to int): Mapping of (featureID, value) to unique integer.\n",
    "        numOHEFeats (int): The number of unique features in the training dataset.\n",
    "\n",
    "    Returns:\n",
    "        LabeledPoint: Contains the label for the observation and the one-hot-encoding of the\n",
    "            raw features based on the provided OHE dictionary.\n",
    "    \"\"\"\n",
    "    splits = point.split(delim)\n",
    "    fields = [ (i,v) for i,v in enumerate(splits[1:]) ]\n",
    "    lp = LabeledPoint(splits[0], oneHotEncoding(fields, OHEDict, numOHEFeats))\n",
    "    return lp\n",
    "\n",
    "def hashFunction(numBuckets, rawFeats, printMapping=False):\n",
    "    \"\"\"Calculate a feature dictionary for an observation's features based on hashing.\n",
    "\n",
    "    Note:\n",
    "        Use printMapping=True for debug purposes and to better understand how the hashing works.\n",
    "\n",
    "    Args:\n",
    "        numBuckets (int): Number of buckets to use as features.\n",
    "        rawFeats (list of (int, str)): A list of features for an observation.  Represented as\n",
    "            (featureID, value) tuples.\n",
    "        printMapping (bool, optional): If true, the mappings of featureString to index will be\n",
    "            printed.\n",
    "\n",
    "    Returns:\n",
    "        dict of int to float:  The keys will be integers which represent the buckets that the\n",
    "            features have been hashed to.  The value for a given key will contain the count of the\n",
    "            (featureID, value) tuples that have hashed to that key.\n",
    "    \"\"\"\n",
    "    mapping = {}\n",
    "    for ind, category in rawFeats:\n",
    "        featureString = category + str(ind)\n",
    "        mapping[featureString] = int(int(hashlib.md5(featureString).hexdigest(), 16) % numBuckets)\n",
    "    if(printMapping): print mapping\n",
    "    sparseFeatures = defaultdict(float)\n",
    "    for bucket in mapping.values():\n",
    "        sparseFeatures[bucket] += 1.0\n",
    "    return dict(sparseFeatures)\n",
    "\n",
    "def parseHashPoint(point, numBuckets, delim):\n",
    "    \"\"\"Create a LabeledPoint for this observation using hashing.\n",
    "\n",
    "    Args:\n",
    "        point (str): A comma separated string where the first value is the label and the rest are\n",
    "            features.\n",
    "        numBuckets: The number of buckets to hash to.\n",
    "\n",
    "    Returns:\n",
    "        LabeledPoint: A LabeledPoint with a label (0.0 or 1.0) and a SparseVector of hashed\n",
    "            features.\n",
    "    \"\"\"\n",
    "    splits = point.split(delim)\n",
    "    fields = [ (i,v) for i,v in enumerate(splits[1:]) ]\n",
    "    vec = SparseVector(numBuckets, hashFunction(numBuckets, fields))\n",
    "    return LabeledPoint(splits[0], vec)\n",
    "\n",
    "def computeLogLoss(p, y):\n",
    "    \"\"\"Calculates the value of log loss for a given probabilty and label.\n",
    "\n",
    "    Note:\n",
    "        log(0) is undefined, so when p is 0 we need to add a small value (epsilon) to it\n",
    "        and when p is 1 we need to subtract a small value (epsilon) from it.\n",
    "\n",
    "    Args:\n",
    "        p (float): A probabilty between 0 and 1.\n",
    "        y (int): A label.  Takes on the values 0 and 1.\n",
    "\n",
    "    Returns:\n",
    "        float: The log loss value.\n",
    "    \"\"\"\n",
    "    epsilon = 10e-12\n",
    "    if p==0:\n",
    "        p+=epsilon\n",
    "    elif p==1:\n",
    "        p-=epsilon\n",
    "    if y==1:\n",
    "        return -log(p)\n",
    "    elif y==0:\n",
    "        return -log(1-p)\n",
    "    else:\n",
    "        raise Exception('y not in {0,1}')\n",
    "\n",
    "def evaluateResults(model, data):\n",
    "    \"\"\"Calculates the log loss for the data given the model.\n",
    "\n",
    "    Args:\n",
    "        model (LogisticRegressionModel): A trained logistic regression model.\n",
    "        data (RDD of LabeledPoint): Labels and features for each observation.\n",
    "\n",
    "    Returns:\n",
    "        float: Log loss for the data.\n",
    "    \"\"\"\n",
    "    probs = data.map(lambda x: (getP(x.features, model.weights, model.intercept), x.label))\n",
    "    logloss = probs.map(lambda x: computeLogLoss(x[0],x[1])).reduce(lambda x,y: x+y) / probs.count()\n",
    "    return logloss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data_location = 's3n://criteo-dataset/rawdata/train'\n",
    "test_data_location = 's3n://criteo-dataset/rawdata/test'\n",
    "validation_data_location = 's3n://criteo-dataset/rawdata/validation'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_data = sc.textFile(train_data_location, 100).cache()\n",
    "validation_data = sc.textFile(validation_data_location, 100).cache()\n",
    "test_data = sc.textFile(test_data_location, 100).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'0\\t1\\t1\\t5\\t0\\t1382\\t4\\t15\\t2\\t181\\t1\\t2\\t\\t2\\t68fd1e64\\t80e26c9b\\tfb936136\\t7b4723c4\\t25c83c98\\t7e0ccccf\\tde7995b8\\t1f89b562\\ta73ee510\\ta8cd5504\\tb2cb9c98\\t37c9c164\\t2824a5f6\\t1adce6ef\\t8ba8b39a\\t891b62e7\\te5ba7672\\tf54016b9\\t21ddcdc9\\tb1252a9d\\t07b5194c\\t\\t3a171ecb\\tc5c50484\\te8b83407\\t9727dd16'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hashTrainData = train_data.map(lambda x: parseHashPoint(x, 1000, '\\t')).cache()\n",
    "hashValidationData = validation_data.map(lambda x: parseHashPoint(x, 1000, '\\t')).cache()\n",
    "hashTestData = test_data.map(lambda x: parseHashPoint(x, 1000, '\\t')).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.classification import LogisticRegressionWithSGD\n",
    "\n",
    "# fixed hyperparameters\n",
    "numIters = 50\n",
    "stepSize = 10.\n",
    "regParam = 0.\n",
    "regType = 'l2'\n",
    "includeIntercept = True\n",
    "\n",
    "starttime = DT.datetime.now()\n",
    "model0 = LogisticRegressionWithSGD.train( data= hashTrainData\n",
    "                                         , iterations= numIters\n",
    "                                         , step= stepSize\n",
    "                                         , regParam= regParam\n",
    "                                         , regType= 'l2'\n",
    "                                         , intercept= includeIntercept )\n",
    "endtime = DT.datetime.now()\n",
    "runtime = (endtime-starttime).seconds\n",
    "runtime_hours = runtime // 3600\n",
    "runtime_minutes = (runtime % 3600) // 60\n",
    "runtime_seconds = (runtime % 3600) % 60\n",
    "print 'job finished in {} hours, {} minutes and {} seconds'.\\\n",
    "        format(runtime_hours, runtime_minutes, runtime_seconds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compute raw scores on the test set\n",
    "predictionAndLabels = test.map(lambda lp: (float(model0.predict(lp.features)), lp.label))\n",
    "\n",
    "# Instantiate metrics object\n",
    "metrics = BinaryClassificationMetrics(predictionAndLabels)\n",
    "\n",
    "# Area under ROC curve\n",
    "print(\"Area under ROC = %s\" % metrics.areaUnderROC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
