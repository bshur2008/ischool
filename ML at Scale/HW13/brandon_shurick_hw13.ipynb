{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# spark functions \n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.linalg import SparseVector\n",
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
    "\n",
    "# helper libraries \n",
    "from collections import defaultdict\n",
    "import hashlib\n",
    "import numpy as np\n",
    "import datetime as DT\n",
    "from math import exp, log\n",
    "\n",
    "# pandas\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def hashFunction(numBuckets, rawFeats, printMapping=False):\n",
    "    \"\"\"Calculate a feature dictionary for an observation's features based on hashing.\n",
    "\n",
    "    Note:\n",
    "        Use printMapping=True for debug purposes and to better understand how the hashing works.\n",
    "\n",
    "    Args:\n",
    "        numBuckets (int): Number of buckets to use as features.\n",
    "        rawFeats (list of (int, str)): A list of features for an observation.  Represented as\n",
    "            (featureID, value) tuples.\n",
    "        printMapping (bool, optional): If true, the mappings of featureString to index will be\n",
    "            printed.\n",
    "\n",
    "    Returns:\n",
    "        dict of int to float:  The keys will be integers which represent the buckets that the\n",
    "            features have been hashed to.  The value for a given key will contain the count of the\n",
    "            (featureID, value) tuples that have hashed to that key.\n",
    "    \"\"\"\n",
    "    mapping = {}\n",
    "    for ind, category in rawFeats:\n",
    "        featureString = category + str(ind)\n",
    "        mapping[featureString] = int(int(hashlib.md5(featureString).hexdigest(), 16) % numBuckets)\n",
    "    if(printMapping): print mapping\n",
    "    sparseFeatures = defaultdict(float)\n",
    "    for bucket in mapping.values():\n",
    "        sparseFeatures[bucket] += 1.0\n",
    "    return dict(sparseFeatures)\n",
    "\n",
    "def parseHashPoint(point, numBuckets, delim):\n",
    "    \"\"\"Create a LabeledPoint for this observation using hashing.\n",
    "\n",
    "    Args:\n",
    "        point (str): A comma separated string where the first value is the label and the rest are\n",
    "            features.\n",
    "        numBuckets: The number of buckets to hash to.\n",
    "\n",
    "    Returns:\n",
    "        LabeledPoint: A LabeledPoint with a label (0.0 or 1.0) and a SparseVector of hashed\n",
    "            features.\n",
    "    \"\"\"\n",
    "    splits = point.split(delim)\n",
    "    fields = [ (i,v) for i,v in enumerate(splits[1:]) ]\n",
    "    vec = SparseVector(numBuckets, hashFunction(numBuckets, fields))\n",
    "    return LabeledPoint(splits[0], vec)\n",
    "\n",
    "def computeLogLoss(p, y):\n",
    "    \"\"\"Calculates the value of log loss for a given probabilty and label.\n",
    "\n",
    "    Note:\n",
    "        log(0) is undefined, so when p is 0 we need to add a small value (epsilon) to it\n",
    "        and when p is 1 we need to subtract a small value (epsilon) from it.\n",
    "\n",
    "    Args:\n",
    "        p (float): A probabilty between 0 and 1.\n",
    "        y (int): A label.  Takes on the values 0 and 1.\n",
    "\n",
    "    Returns:\n",
    "        float: The log loss value.\n",
    "    \"\"\"\n",
    "    epsilon = 10e-12\n",
    "    if p==0:\n",
    "        p+=epsilon\n",
    "    elif p==1:\n",
    "        p-=epsilon\n",
    "    if y==1:\n",
    "        return -log(p)\n",
    "    elif y==0:\n",
    "        return -log(1-p)\n",
    "    else:\n",
    "        raise Exception('y not in {0,1}')\n",
    "\n",
    "def evaluateResults(model, data):\n",
    "    \"\"\"Calculates the log loss for the data given the model.\n",
    "\n",
    "    Args:\n",
    "        model (LogisticRegressionModel): A trained logistic regression model.\n",
    "        data (RDD of LabeledPoint): Labels and features for each observation.\n",
    "\n",
    "    Returns:\n",
    "        float: Log loss for the data.\n",
    "    \"\"\"\n",
    "    probs = data.map(lambda x: (getP(x.features, model.weights, model.intercept), x.label))\n",
    "    logloss = probs.map(lambda x: computeLogLoss(x[0],x[1])).reduce(lambda x,y: x+y) / probs.count()\n",
    "    return logloss\n",
    "\n",
    "def getP(x, w, intercept):\n",
    "    \"\"\"Calculate the probability for an observation given a set of weights and intercept.\n",
    "\n",
    "    Note:\n",
    "        We'll bound our raw prediction between 20 and -20 for numerical purposes.\n",
    "\n",
    "    Args:\n",
    "        x (SparseVector): A vector with values of 1.0 for features that exist in this\n",
    "            observation and 0.0 otherwise.\n",
    "        w (DenseVector): A vector of weights (betas) for the model.\n",
    "        intercept (float): The model's intercept.\n",
    "\n",
    "    Returns:\n",
    "        float: A probability between 0 and 1.\n",
    "    \"\"\"\n",
    "    rawPrediction = x.dot(w)+intercept\n",
    "\n",
    "    # Bound the raw prediction value\n",
    "    rawPrediction = min(rawPrediction, 20)\n",
    "    rawPrediction = max(rawPrediction, -20)\n",
    "    return 1.0/(1+exp(-rawPrediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data_location = 's3n://criteo-dataset/rawdata/train'\n",
    "test_data_location = 's3n://criteo-dataset/rawdata/test'\n",
    "validation_data_location = 's3n://criteo-dataset/rawdata/validation'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data = sc.textFile(train_data_location).cache()\n",
    "validation_data = sc.textFile(validation_data_location).cache()\n",
    "test_data = sc.textFile(test_data_location).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hashTrainData = train_data.map(lambda x: parseHashPoint(x, 1000, '\\t')).cache()\n",
    "hashValidationData = validation_data.map(lambda x: parseHashPoint(x, 1000, '\\t')).cache()\n",
    "hashTestData = test_data.map(lambda x: parseHashPoint(x, 1000, '\\t')).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "job finished in 0 hours, 15 minutes and 5 seconds\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.classification import LogisticRegressionWithSGD\n",
    "\n",
    "# fixed hyperparameters\n",
    "numIters = 50\n",
    "stepSize = 10.\n",
    "regParam = 0.\n",
    "regType = 'l2'\n",
    "includeIntercept = True\n",
    "\n",
    "starttime = DT.datetime.now()\n",
    "model0 = LogisticRegressionWithSGD.train( data= hashTrainData\n",
    "                                         , iterations= numIters\n",
    "                                         , step= stepSize\n",
    "                                         , regParam= regParam\n",
    "                                         , regType= 'l2'\n",
    "                                         , intercept= includeIntercept )\n",
    "endtime = DT.datetime.now()\n",
    "runtime = (endtime-starttime).seconds\n",
    "runtime_hours = runtime // 3600\n",
    "runtime_minutes = (runtime % 3600) // 60\n",
    "runtime_seconds = (runtime % 3600) % 60\n",
    "print 'job finished in {} hours, {} minutes and {} seconds'.\\\n",
    "        format(runtime_hours, runtime_minutes, runtime_seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EMR Setup:\n",
    "<table style='align: left'>\n",
    "<tr>\n",
    "<td><strong>Node type</strong></td>\n",
    "<td><strong>Count</strong></td>\n",
    "<td><strong>Intance</strong></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Master</td>\n",
    "<td>1</td>\n",
    "<td>r3.xlarge</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Core</td>\n",
    "<td>7</td>\n",
    "<td>r3.xlarge</td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Area under ROC, Train = 0.651420018788\n",
      "Area under ROC, Validation = 0.651397081537\n",
      "Area under ROC, Test = 0.651683988549\n"
     ]
    }
   ],
   "source": [
    "# Compute raw scores on the validation set\n",
    "predictionAndLabelsTrain = hashTrainData.map(lambda lp: (float(model0.predict(lp.features)), lp.label))\n",
    "predictionAndLabelsVal = hashValidationData.map(lambda lp: (float(model0.predict(lp.features)), lp.label))\n",
    "predictionAndLabelsTest = hashTestData.map(lambda lp: (float(model0.predict(lp.features)), lp.label))\n",
    "\n",
    "# Instantiate metrics object\n",
    "metricsTrain = BinaryClassificationMetrics(predictionAndLabelsTrain)\n",
    "metricsVal = BinaryClassificationMetrics(predictionAndLabelsVal)\n",
    "metricsTest = BinaryClassificationMetrics(predictionAndLabelsTest)\n",
    "\n",
    "# Area under ROC curve\n",
    "print(\"Area under ROC, Train = %s\" % metricsTrain.areaUnderROC)\n",
    "print(\"Area under ROC, Validation = %s\" % metricsVal.areaUnderROC)\n",
    "print(\"Area under ROC, Test = %s\" % metricsTest.areaUnderROC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log-loss, Train = 0.556543482585\n",
      "Log-loss, Validation = 0.556640268951\n",
      "Log-loss, Test = 0.556543482585\n"
     ]
    }
   ],
   "source": [
    "# Evaluate Log-Loss\n",
    "loglossTrain = evaluateResults(model0,hashTrainData)\n",
    "loglossVal = evaluateResults(model0,hashValidationData)\n",
    "loglossTest = evaluateResults(model0,hashTrainData)\n",
    "\n",
    "# Log-loss results\n",
    "print(\"Log-loss, Train = %s\" % loglossTrain)\n",
    "print(\"Log-loss, Validation = %s\" % loglossVal)\n",
    "print(\"Log-loss, Test = %s\" % loglossTest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model results table\n",
    "<table>\n",
    "<tr>\n",
    "<th>Metric</th>\n",
    "<th>Dataset</th>\n",
    "<th>Result</th>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>AUC</td>\n",
    "<td>Train</td>\n",
    "<td>65.14%</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>AUC</td>\n",
    "<td>Validation</td>\n",
    "<td>65.14%</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>AUC</td>\n",
    "<td>Test</td>\n",
    "<td>65.17%</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Log-loss</td>\n",
    "<td>Train</td>\n",
    "<td>55.65%</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Log-loss</td>\n",
    "<td>Validation</td>\n",
    "<td>55.66%</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Log-loss</td>\n",
    "<td>Test</td>\n",
    "<td>55.65%</td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW 13.5: Criteo Phase 2 hyperparameter tuning  \n",
    ">SPECIAL NOTE:\n",
    "Please share your findings as they become available with class via the Google Group. You will get brownie points for this.  Once results are shared please used them and build on them.\n",
    " \n",
    "\n",
    ">Using the training dataset, validation dataset and testing dataset in the Criteo bucket perform the following experiments:\n",
    "\n",
    ">-- write spark code (borrow from Phase 1 of this project) to train a logistic regression model with various hyperparamters. Do a gridsearch of the hyperparameter space and determine optimal settings using the validation set.\n",
    "\n",
    ">-- Number of buckets for hashing: 1,000, 10,000, .... explore different values  here\n",
    "-- Logistic Regression: regularization term: [1e-6, 1e-3]  explore other  values here also\n",
    "-- Logistic Regression: step size: explore different step sizes. Focus on a stepsize of 1 initially. \n",
    "\n",
    ">Report the AWS cluster configuration that you used and how long in minutes and seconds it takes to complete this job.\n",
    "\n",
    ">Report in tabular form and using heatmaps the AUC values (https://en.wikipedia.org/wiki/Receiver_operating_characteristic) for the Training, Validation, and Testing datasets.\n",
    "Report in tabular form and using heatmaps  the logLossTest for the Training, Validation, and Testing datasets.\n",
    "\n",
    ">Dont forget to put a caption on your tables (above the table) and on your heatmap figures (put caption below figures) detailing the experiment associated with each table or figure (data, algorithm used, parameters and settings explored.\n",
    "\n",
    ">Discuss the optimal setting to solve this problem  in terms of the following:\n",
    "-- Features\n",
    "-- Learning algortihm\n",
    "-- Spark cluster\n",
    "\n",
    ">Justiy your recommendations based on your experimental results and cross reference with table numbers and figure numbers. Also highlight key results with annotations, both textual and line and box based, on your tables and graphs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "job finished in 1 hours, 24 minutes and 27 seconds\n"
     ]
    }
   ],
   "source": [
    "hashBuckets = [ 1000, 5000, 10000 ]\n",
    "regTerms = [ 1e-6, 1e-3, 1e-2] \n",
    "stepSizes = [ 1, 10, 100 ]\n",
    "results = {}\n",
    "results['hashBuckets'] = []\n",
    "results['regTerms'] = []\n",
    "results['stepSizes'] = []\n",
    "results['validationAUC'] = []\n",
    "results['validationLogLoss'] = []\n",
    "\n",
    "starttime = DT.datetime.now()\n",
    "for h in hashBuckets:\n",
    "    # load data\n",
    "    hashTrainData = train_data.map(lambda x: parseHashPoint(x, h, '\\t')).cache()\n",
    "    hashValidationData = validation_data.map(lambda x: parseHashPoint(x, h, '\\t')).cache()\n",
    "    \n",
    "    for r in regTerms:\n",
    "        for s in stepSizes:\n",
    "            results['hashBuckets'].append(h)\n",
    "            results['regTerms'].append(r)\n",
    "            results['stepSizes'].append(s)\n",
    "            \n",
    "            # run model \n",
    "            model = LogisticRegressionWithSGD.train( data= hashTrainData\n",
    "                                         , iterations= 10\n",
    "                                         , step= s\n",
    "                                         , regParam= r\n",
    "                                         , regType= 'l2'\n",
    "                                         , intercept= True )\n",
    "            \n",
    "            # calculation binary classification metrics \n",
    "            predictionAndLabelsVal = hashValidationData. \\\n",
    "                                        map(lambda lp: (float(model.predict(lp.features)), lp.label))\n",
    "            metricsVal = BinaryClassificationMetrics(predictionAndLabelsVal)\n",
    "            \n",
    "            # calculate log-loss\n",
    "            loglossVal = evaluateResults(model, hashValidationData)\n",
    "            \n",
    "            # store results\n",
    "            results['validationAUC'].append(metricsVal.areaUnderROC)\n",
    "            results['validationLogLoss'].append(loglossVal)\n",
    "            \n",
    "endtime = DT.datetime.now()\n",
    "runtime = (endtime-starttime).seconds\n",
    "runtime_hours = runtime // 3600\n",
    "runtime_minutes = (runtime % 3600) // 60\n",
    "runtime_seconds = (runtime % 3600) % 60\n",
    "print 'job finished in {} hours, {} minutes and {} seconds'.\\\n",
    "        format(runtime_hours, runtime_minutes, runtime_seconds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hashBuckets</th>\n",
       "      <th>regTerms</th>\n",
       "      <th>stepSizes</th>\n",
       "      <th>validationAUC</th>\n",
       "      <th>validationLogLoss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>1</td>\n",
       "      <td>0.500208</td>\n",
       "      <td>0.543713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>10</td>\n",
       "      <td>0.617106</td>\n",
       "      <td>1.128304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1000</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>100</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>5.126996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.500199</td>\n",
       "      <td>0.543741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>10</td>\n",
       "      <td>0.500001</td>\n",
       "      <td>1.610130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>100</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>5.126856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1000</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.500151</td>\n",
       "      <td>0.543995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1000</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>10</td>\n",
       "      <td>0.501479</td>\n",
       "      <td>1.066049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1000</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>100</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>5.126996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>5000</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>1</td>\n",
       "      <td>0.500485</td>\n",
       "      <td>0.543310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>5000</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>10</td>\n",
       "      <td>0.556992</td>\n",
       "      <td>0.649618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>5000</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>100</td>\n",
       "      <td>0.502956</td>\n",
       "      <td>4.725897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>5000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.500471</td>\n",
       "      <td>0.543338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>5000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>10</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>2.629210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>5000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>100</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>5.125339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>5000</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.500372</td>\n",
       "      <td>0.543593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>5000</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>10</td>\n",
       "      <td>0.565821</td>\n",
       "      <td>1.261521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>5000</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>100</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>5.126966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>10000</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>1</td>\n",
       "      <td>0.500542</td>\n",
       "      <td>0.543222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>10000</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>10</td>\n",
       "      <td>0.539296</td>\n",
       "      <td>0.698795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>10000</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>100</td>\n",
       "      <td>0.505383</td>\n",
       "      <td>4.602265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>10000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.500527</td>\n",
       "      <td>0.543250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>10000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>10</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>2.530326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>10000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>100</td>\n",
       "      <td>0.500001</td>\n",
       "      <td>5.121262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>10000</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.500415</td>\n",
       "      <td>0.543504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>10000</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>10</td>\n",
       "      <td>0.650039</td>\n",
       "      <td>0.661922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>10000</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>100</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>5.126948</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    hashBuckets  regTerms  stepSizes  validationAUC  validationLogLoss\n",
       "0          1000  0.000001          1       0.500208           0.543713\n",
       "1          1000  0.000001         10       0.617106           1.128304\n",
       "2          1000  0.000001        100       0.500000           5.126996\n",
       "3          1000  0.001000          1       0.500199           0.543741\n",
       "4          1000  0.001000         10       0.500001           1.610130\n",
       "5          1000  0.001000        100       0.500000           5.126856\n",
       "6          1000  0.010000          1       0.500151           0.543995\n",
       "7          1000  0.010000         10       0.501479           1.066049\n",
       "8          1000  0.010000        100       0.500000           5.126996\n",
       "9          5000  0.000001          1       0.500485           0.543310\n",
       "10         5000  0.000001         10       0.556992           0.649618\n",
       "11         5000  0.000001        100       0.502956           4.725897\n",
       "12         5000  0.001000          1       0.500471           0.543338\n",
       "13         5000  0.001000         10       0.500000           2.629210\n",
       "14         5000  0.001000        100       0.500000           5.125339\n",
       "15         5000  0.010000          1       0.500372           0.543593\n",
       "16         5000  0.010000         10       0.565821           1.261521\n",
       "17         5000  0.010000        100       0.500000           5.126966\n",
       "18        10000  0.000001          1       0.500542           0.543222\n",
       "19        10000  0.000001         10       0.539296           0.698795\n",
       "20        10000  0.000001        100       0.505383           4.602265\n",
       "21        10000  0.001000          1       0.500527           0.543250\n",
       "22        10000  0.001000         10       0.500000           2.530326\n",
       "23        10000  0.001000        100       0.500001           5.121262\n",
       "24        10000  0.010000          1       0.500415           0.543504\n",
       "25        10000  0.010000         10       0.650039           0.661922\n",
       "26        10000  0.010000        100       0.500000           5.126948"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hashBuckets</th>\n",
       "      <th>regTerms</th>\n",
       "      <th>stepSizes</th>\n",
       "      <th>validationAUC</th>\n",
       "      <th>validationLogLoss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>10000</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>1</td>\n",
       "      <td>0.500542</td>\n",
       "      <td>0.543222</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    hashBuckets  regTerms  stepSizes  validationAUC  validationLogLoss\n",
       "18        10000  0.000001          1       0.500542           0.543222"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df = pd.DataFrame(results)\n",
    "bestresult = result_df['validationLogLoss']==np.min(result_df['validationLogLoss'])\n",
    "result_df[bestresult]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# use best results\n",
    "hashBuckets = 10000\n",
    "regTerm = 0.000001\n",
    "stepSize = 1\n",
    "\n",
    "# build datasets with best grid-search results\n",
    "hashTrainData = train_data.map(lambda x: parseHashPoint(x, hashBuckets, '\\t')).cache()\n",
    "hashValidationData = validation_data.map(lambda x: parseHashPoint(x, hashBuckets, '\\t')).cache()\n",
    "hashTestData = test_data.map(lambda x: parseHashPoint(x, hashBuckets, '\\t')).cache()\n",
    "\n",
    "# create LR model \n",
    "modelGS = LogisticRegressionWithSGD.train( data= hashTrainData\n",
    "                                         , iterations= 100\n",
    "                                         , step= stepSize\n",
    "                                         , regParam= regTerm\n",
    "                                         , regType= 'l2'\n",
    "                                         , intercept= True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get model results for all datasets\n",
    "predictionAndLabelsTrain = hashTrainData. \\\n",
    "                            map(lambda lp: (float(modelGS.predict(lp.features)), lp.label))\n",
    "predictionAndLabelsVal = hashValidationData. \\\n",
    "                            map(lambda lp: (float(modelGS.predict(lp.features)), lp.label))\n",
    "predictionAndLabelsTest = hashTestData. \\\n",
    "                            map(lambda lp: (float(modelGS.predict(lp.features)), lp.label))\n",
    "metricsTrain = BinaryClassificationMetrics(predictionAndLabelsTrain)\n",
    "metricsVal = BinaryClassificationMetrics(predictionAndLabelsVal)\n",
    "metricsTest = BinaryClassificationMetrics(predictionAndLabelsTest)\n",
    "\n",
    "# calculate log-loss\n",
    "loglossTrain = evaluateResults(modelGS, hashTrainData)\n",
    "loglossVal = evaluateResults(modelGS, hashValidationData)\n",
    "loglossTest = evaluateResults(modelGS, hashTestData)\n",
    "\n",
    "# store results\n",
    "bestresults = {}\n",
    "bestresults['hashBuckets'] = hashBuckets\n",
    "bestresults['regTerm'] = regTerm\n",
    "bestresults['stepSize'] = stepSize\n",
    "bestresults['AUCtrain'] = metricsTrain.areaUnderROC\n",
    "bestresults['AUCvalidation'] = metricsVal.areaUnderROC\n",
    "bestresults['AUCtest'] = metricsTest.areaUnderROC\n",
    "bestresults['LogLosstrain'] = loglossTrain\n",
    "bestresults['LogLossvalidation'] = loglossVal\n",
    "bestresults['LogLosstest'] = loglossTest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'AUCtest': 0.5272941767928676,\n",
       " 'AUCtrain': 0.5271317711642737,\n",
       " 'AUCvalidation': 0.5273112353305975,\n",
       " 'LogLosstest': 0.5218620652196896,\n",
       " 'LogLosstrain': 0.521725934571774,\n",
       " 'LogLossvalidation': 0.5219043589740324,\n",
       " 'hashBuckets': 10000,\n",
       " 'regTerm': 1e-06,\n",
       " 'stepSize': 1}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bestresults"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
