{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework 1: Brandon Shurick"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW1.0.0\n",
    "Define big data. Provide an example of a big data problem in your domain of expertise. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In my opinion, \"big data\" is the integration of data into core business processes (regardless of data size). Big data manifests itself in two major ways in my domain of expertise (business intelligence & operations):\n",
    "- recommendations based on observational data, i.e. for account managers, customer care agents, based on client or partner history\n",
    "- causational interpretation of randomized, controlled experiments, i.e. A-B tests\n",
    "\n",
    "Each of these requires careful recording and cleansing of data that may involve any of the \"3 V's\": volume, variety, velocity. For example, we may need to use cheap storage and Hadoop in order to store all of our operational and customer usage data, because we don't always know what features we might need to utilize for recommendations based on observational data. We might need to build complicated data processing code to cleans and connect all of the types of data we record. Lastly, we might need to build infrastructure that can support realtime analysis at high-speed. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW1.0.1\n",
    "In 500 words (English or pseudo code or a combination) describe how to estimate the bias, the variance, the irreduciable error for a test dataset T when using polynomial regression models of degree 1, 2,3, 4,5 are considered. How would you select a model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, to estimate bias it is necessary to have a target function defined that the regression models are attemtping to estimate. We can call this function $f(x)$. Then a regression model $g_N(x)$ should be created from training data to approximate $f(x)$ for a dataset $T$ over each of the $N$ polynomial degrees being considered, where $T$ represents the result of function $f(x)$ plus some additional random noise. \n",
    "\n",
    "The bias would then be calculated as the mean of the squared mean test error (as measured by mean squared error in regression) across all datasets for each polynomial degree being considered, minus the result of the true function $f(x)$. In pseudocode:\n",
    "\n",
    "polydegree = {}  \n",
    "bias = {}  \n",
    "for n in N:  \n",
    "&nbsp;&nbsp;for d in T:  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;if n not in polydegree: polydegree[n]=[ ]  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;error_d = (g_n(d)-d)\\*\\*2  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;polydegree[n].append(error_d)  \n",
    "for n in N:  \n",
    "&nbsp;&nbsp;bias[n] = mean((mean(polydegree[n])-f(x))\\*\\*2)  \n",
    "\n",
    "Measuring the variance would not require the function $f(x)$ but would simply be a calculation of the mean variance of the model $g_N(x)$ test results for each polynomial degree being considered. In pseudocode:\n",
    "\n",
    "polydegree = {}  \n",
    "variance = {}  \n",
    "for n in N:  \n",
    "&nbsp;&nbsp;for d in T:  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;if n not in polydegree: polydegree[n]=[ ]  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;result_d = g_n(d)  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;polydegree[n].append(result_d)  \n",
    "for n in N:  \n",
    "&nbsp;&nbsp;results = [ (d-mean(polydegree[n]))\\*\\*2 for d in polydegree[n] ]  \n",
    "&nbsp;&nbsp;variance[n] = mean(results)\n",
    "\n",
    "The irreducable (constant) error does not depend on the model $g_N(x)$, and is calculated as the mean squared difference between the observations in the dataset $T$ from the true function $f(x)$. In pseudocode:\n",
    "\n",
    "polydegree = {}  \n",
    "error = {}  \n",
    "for n in N:  \n",
    "&nbsp;&nbsp;for d in T:  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;if n not in polydegree: polydegree[n]=[ ]  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;result_d = (d-f(x))\\*\\*2  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;polydegree[n].append(result_d)  \n",
    "for n in N:  \n",
    "&nbsp;&nbsp;error[n] = mean(polydegree[n])\n",
    "\n",
    "To choose the model, I would select the model $g_N(x)$ which has the lowest combined variance and bias (ignoring the error/noise of the data, which is constant across all models). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW1.1\n",
    "Read through the provided control script (pNaiveBayes.sh)\n",
    "   and all of its comments. When you are comfortable with their\n",
    "   purpose and function, respond to the remaining homework questions below. \n",
    "   A simple cell in the notebook with a print statmement with  a \"done\" string will suffice here. (dont forget to include the Question Number and the quesition in the cell as a multiline comment!)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW1.2\n",
    "Provide a mapper/reducer pair that, when executed by pNaiveBayes.sh\n",
    "   will determine the number of occurrences of a single, user-specified word. Examine the word “assistance” and report your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/env python\n",
    "import re, sys\n",
    "filename = sys.argv[1]\n",
    "findword = sys.argv[2]\n",
    "WORDS = re.compile(r'[\\w]+')\n",
    "for line in open(filename,'r').readlines():\n",
    "    line = line.strip()\n",
    "    wordslist = WORDS.findall(line)\n",
    "    findwords = [w for w in wordslist if w==findword ]\n",
    "    print(len(findwords))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!chmod +x mapper.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/env python\n",
    "import sys\n",
    "filenames = sys.argv[1:]\n",
    "sums = []\n",
    "for f in filenames:\n",
    "    for line in open(f,'r').readlines():\n",
    "        line = line.strip()\n",
    "        sums.append(int(line))\n",
    "print(sum(sums))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod +x reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod +x pNaiveBayes.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\r\n"
     ]
    }
   ],
   "source": [
    "!./pNaiveBayes.sh 4 assistance\n",
    "!cat *.output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
