{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework 1: Brandon Shurick"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW1.0.0\n",
    "Define big data. Provide an example of a big data problem in your domain of expertise. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In my opinion, \"big data\" is the integration of data into core business processes (regardless of data size). Big data manifests itself in two major ways in my domain of expertise (business intelligence & operations):\n",
    "- recommendations based on observational data, i.e. for account managers, customer care agents, other internal employees, based on client or partner history\n",
    "- causational interpretation of randomized, controlled experiments, i.e. A-B tests\n",
    "\n",
    "Each of these requires careful recording and cleansing of data that may involve any of the \"3 V's\": volume, variety, velocity. For example, we may need to use cheap storage and Hadoop in order to store all of our operational and customer usage data, because we don't always know what features we might need to utilize for recommendations based on observational data. We might need to build complicated data processing code to cleans and connect all of the types of data we record. Lastly, we might need to build infrastructure that can support realtime analysis at high-speed. \n",
    "\n",
    "In terms of data size, processing data on a single node (i.e. laptop) becomes difficult when the data fills up the harddrive, say 1-terabyte of data, and machine-learning algorithms will have trouble on a single node when the data size is more than a few gigabytes. At this point, processing requires multiple nodes, or expensive vertical scaling. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW1.0.1\n",
    "In 500 words (English or pseudo code or a combination) describe how to estimate the bias, the variance, the irreduciable error for a test dataset T when using polynomial regression models of degree 1, 2,3, 4,5 are considered. How would you select a model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, to estimate bias it is necessary to have a target function defined that the regression models are attemtping to estimate. We can call this function $f(x)$. Then a regression model $g_N(x)$ should be created from training data to approximate $f(x)$ for a dataset $T$ over each of the $N$ polynomial degrees being considered, where $T$ represents the result of function $f(x)$ plus some additional random noise. \n",
    "\n",
    "The bias would then be calculated as the mean of the squared mean test error (as measured by mean squared error in regression) across all datasets for each polynomial degree being considered, minus the result of the true function $f(x)$. In pseudocode:\n",
    "\n",
    "polydegree = {}  \n",
    "bias = {}  \n",
    "for n in N:  \n",
    "&nbsp;&nbsp;for d in T:  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;if n not in polydegree: polydegree[n]=[ ]  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;error_d = (g_n(d)-d)\\*\\*2  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;polydegree[n].append(error_d)  \n",
    "for n in N:  \n",
    "&nbsp;&nbsp;bias[n] = mean((mean(polydegree[n])-f(x))\\*\\*2)  \n",
    "\n",
    "Measuring the variance would not require the function $f(x)$ but would simply be a calculation of the mean variance of the model $g_N(x)$ test results for each polynomial degree being considered. In pseudocode:\n",
    "\n",
    "polydegree = {}  \n",
    "variance = {}  \n",
    "for n in N:  \n",
    "&nbsp;&nbsp;for d in T:  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;if n not in polydegree: polydegree[n]=[ ]  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;result_d = g_n(d)  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;polydegree[n].append(result_d)  \n",
    "for n in N:  \n",
    "&nbsp;&nbsp;results = [ (d-mean(polydegree[n]))\\*\\*2 for d in polydegree[n] ]  \n",
    "&nbsp;&nbsp;variance[n] = mean(results)\n",
    "\n",
    "The irreducable (constant) error does not depend on the model $g_N(x)$, and is calculated as the mean squared difference between the observations in the dataset $T$ from the true function $f(x)$. In pseudocode:\n",
    "\n",
    "polydegree = {}  \n",
    "error = {}  \n",
    "for n in N:  \n",
    "&nbsp;&nbsp;for d in T:  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;if n not in polydegree: polydegree[n]=[ ]  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;result_d = (d-f(x))\\*\\*2  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;polydegree[n].append(result_d)  \n",
    "for n in N:  \n",
    "&nbsp;&nbsp;error[n] = mean(polydegree[n])\n",
    "\n",
    "The math behind this would be that the Expected Error = $E[(g(x)-\\bar{g}(x))^2] + (\\bar{g}(x)-f(x))^2 + E[(y-f(x))^2]$, where the component $E[(g(x)-\\bar{g}(x))^2]$ represents the variance, the component (\\bar{g}(x)-f(x))^2 represents the bias (squared), and the last component represents the irreducible noise. \n",
    "\n",
    "To choose the model, I would select the model $g_N(x)$ which has the lowest variance and lowest bias (ignoring the error/noise of the data, which is constant across all models). In the below image, for example, the lowest combination of bias and variance is the minimum point of $bias^2+variance$, or at the 3rd polynomial order.\n",
    "\n",
    "<img src='http://inside-bigdata.com/wp-content/uploads/2014/10/Bia_variance_tradeoff_fig.jpg' style='height: 250px;' align='left'></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW1.1\n",
    "Read through the provided control script (pNaiveBayes.sh)\n",
    "   and all of its comments. When you are comfortable with their\n",
    "   purpose and function, respond to the remaining homework questions below. \n",
    "   A simple cell in the notebook with a print statmement with  a \"done\" string will suffice here. (dont forget to include the Question Number and the quesition in the cell as a multiline comment!)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW1.2\n",
    "Provide a mapper/reducer pair that, when executed by pNaiveBayes.sh\n",
    "   will determine the number of occurrences of a single, user-specified word. Examine the word “assistance” and report your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/env python\n",
    "import re, sys\n",
    "filename = sys.argv[1]\n",
    "findword = sys.argv[2]\n",
    "WORDS = re.compile(r'[\\w]+')\n",
    "for line in open(filename,'r').readlines():\n",
    "    line = line.strip()\n",
    "    wordslist = WORDS.findall(line)\n",
    "    findwords = [w for w in wordslist if w==findword ]\n",
    "    print(findword+'\\t'+str(len(findwords))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!chmod +x mapper.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/env python\n",
    "import sys\n",
    "filenames = sys.argv[1:]\n",
    "sums = []\n",
    "findword = ''\n",
    "for f in filenames:\n",
    "    for line in open(f,'r').readlines():\n",
    "        line = line.strip()\n",
    "        findword,wordcount = line.split('\\t')\n",
    "        sums.append(int(wordcount))\n",
    "print(findword+': '+str(sum(sums)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod +x reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod +x pNaiveBayes.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assistance: 10\r\n"
     ]
    }
   ],
   "source": [
    "!./pNaiveBayes.sh 4 assistance\n",
    "!cat *.output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW1.3. \n",
    "Provide a mapper/reducer pair that, when executed by pNaiveBayes.sh\n",
    "   will classify the email messages by a single, user-specified word using the multinomial Naive Bayes Formulation. Examine the word “assistance” and report your results. To do so, make sure that\n",
    "   \n",
    "   - mapper.py and\n",
    "   - reducer.py \n",
    "\n",
    "That performs a single word Naive Bayes classification. For multinomial Naive Bayes, the Pr(X=“assistance”|Y=SPAM) is calculated as follows:\n",
    "\n",
    "  the number of times “assistance” occurs in SPAM labeled documents / the number of words in documents labeled SPAM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/env python\n",
    "import re, sys\n",
    "filename = sys.argv[1]\n",
    "findword = sys.argv[2]\n",
    "WORDS = re.compile(r'[\\w]+')\n",
    "for line in open(filename,'r').readlines():\n",
    "    \n",
    "    ## Read lines from data chunk ##\n",
    "    # Remove non-word, non-whitespace characters\n",
    "    line = re.sub(r'[^\\w\\s]+','',line.strip())\n",
    "    components = line.split('\\t')\n",
    "    \n",
    "    ## Only process valid records ##\n",
    "    try:\n",
    "        spamdoc = int(components[1])\n",
    "    except IndexError:\n",
    "        continue \n",
    "    \n",
    "    ## Compile list of words ##\n",
    "    words = ' '.join(components[2:])\n",
    "    wordslist = WORDS.findall(words)\n",
    "    \n",
    "    ## Find words based on user input ##\n",
    "    findwords = [ w for w in wordslist if w==findword ]\n",
    "    \n",
    "    ## Count total words in document ##\n",
    "    totalwords = len(wordslist)\n",
    "    \n",
    "    ## Send results to reducer ##\n",
    "    print('{}\\t{}\\t{}\\t{}'.format(components[0],spamdoc,len(findwords),totalwords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod +x mapper.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/env python\n",
    "import sys\n",
    "import math\n",
    "filenames = sys.argv[1:]\n",
    "docs = 0\n",
    "typecnt = {}\n",
    "prior = {}\n",
    "findwords = {}\n",
    "totalwords = {}\n",
    "\n",
    "for f in filenames:\n",
    "    for line in open(f,'r').readlines():\n",
    "        \n",
    "        ## Read in lines from Mapper ##\n",
    "        line = line.strip()\n",
    "        components = line.split('\\t')\n",
    "        cid,spam,findword_cnt,totalword_cnt = components\n",
    "        \n",
    "        ## Increment counts ##\n",
    "        # Total documents\n",
    "        docs += 1\n",
    "        \n",
    "        # Count of class \n",
    "        if spam not in typecnt: typecnt[spam] = 1\n",
    "        else: typecnt[spam] += 1\n",
    "        \n",
    "        # Count of words matching user input\n",
    "        if spam not in findwords: findwords[spam] = int(findword_cnt)\n",
    "        else: findwords[spam] += int(findword_cnt)\n",
    "        \n",
    "        # Count of all words \n",
    "        if spam not in totalwords: totalwords[spam] = int(totalword_cnt)\n",
    "        else: totalwords[spam] += int(totalword_cnt)\n",
    "        \n",
    "        ## Make prediction based on data available at time of first pass ##\n",
    "        if '1' not in typecnt:\n",
    "            print('{}\\t{}\\t{}'.format(cid,0,spam))  \n",
    "        else:\n",
    "            # Recalculate probability given inputted word\n",
    "            logprior = math.log((typecnt['1']*1.0 / docs)/(typecnt['0']*1.0 / docs))\n",
    "            spamp = (findwords['1']*1.0/totalwords['1'])**int(findword_cnt)\n",
    "            nonspamp = (findwords['0']*1.0/totalwords['0'])**int(findword_cnt)\n",
    "            logp = math.log(spamp/nonspamp) if nonspamp>0 else 0 \n",
    "            if (logp+logprior)>0: \n",
    "                # If sum is greater than zero, predict spam\n",
    "                print('{}\\t{}\\t{}'.format(cid,1,spam))\n",
    "            else: \n",
    "                # Otherwise predict ham\n",
    "                print('{}\\t{}\\t{}'.format(cid,0,spam))      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod +x reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod +x pNaiveBayes.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "000119991210farmer\t0\t0\n",
      "000119991210kaminski\t0\t0\n",
      "000120000117beck\t0\t0\n",
      "000120000606lokay\t0\t0\n",
      "000120010207kitchen\t0\t0\n",
      "000120010402williams\t0\t0\n",
      "000219991213farmer\t0\t0\n",
      "000220010207kitchen\t0\t0\n",
      "000220010525SA_and_HP\t0\t1\n",
      "000220031218GP\t0\t1\n",
      "000220040801BG\t0\t1\n",
      "000319991210kaminski\t0\t0\n",
      "000319991214farmer\t0\t0\n",
      "000320000117beck\t0\t0\n",
      "000320010208kitchen\t0\t0\n",
      "000320031218GP\t0\t1\n",
      "000320040801BG\t0\t1\n",
      "000419991210kaminski\t1\t0\n",
      "000419991214farmer\t0\t0\n",
      "000420010402williams\t0\t0\n",
      "000420010612SA_and_HP\t0\t1\n",
      "000420040801BG\t0\t1\n",
      "000519991212kaminski\t0\t0\n",
      "000519991214farmer\t0\t0\n",
      "000520000606lokay\t0\t0\n",
      "000520010208kitchen\t0\t0\n",
      "000520010623SA_and_HP\t0\t1\n",
      "000520031218GP\t0\t1\n",
      "000619991213kaminski\t0\t0\n",
      "000620010208kitchen\t0\t0\n",
      "000620010403williams\t0\t0\n",
      "000620010625SA_and_HP\t0\t1\n",
      "000620031218GP\t0\t1\n",
      "000620040801BG\t0\t1\n",
      "000719991213kaminski\t0\t0\n",
      "000719991214farmer\t0\t0\n",
      "000720000117beck\t0\t0\n",
      "000720010209kitchen\t0\t0\n",
      "000720031218GP\t0\t1\n",
      "000720040801BG\t0\t1\n",
      "000820010209kitchen\t0\t0\n",
      "000820010612SA_and_HP\t0\t1\n",
      "000820010625SA_and_HP\t0\t1\n",
      "000820031218GP\t0\t1\n",
      "000820040801BG\t0\t1\n",
      "000919991213kaminski\t0\t0\n",
      "000919991214farmer\t0\t0\n",
      "000920000607lokay\t0\t0\n",
      "000920010209kitchen\t0\t0\n",
      "000920010626SA_and_HP\t0\t1\n",
      "000920031218GP\t0\t1\n",
      "001019991214farmer\t0\t0\n",
      "001019991214kaminski\t0\t0\n",
      "001020010209kitchen\t0\t0\n",
      "001020010628SA_and_HP\t1\t1\n",
      "001020031218GP\t0\t1\n",
      "001020040801BG\t0\t1\n",
      "001119991214farmer\t0\t0\n",
      "001120010628SA_and_HP\t0\t1\n",
      "001120010629SA_and_HP\t0\t1\n",
      "001120031218GP\t0\t1\n",
      "001120040801BG\t0\t1\n",
      "001219991214farmer\t0\t0\n",
      "001219991214kaminski\t0\t0\n",
      "001220000117beck\t0\t0\n",
      "001220000608lokay\t0\t0\n",
      "001220010209kitchen\t0\t0\n",
      "001220031219GP\t0\t1\n",
      "001319991214farmer\t0\t0\n",
      "001319991214kaminski\t0\t0\n",
      "001320010403williams\t0\t0\n",
      "001320010630SA_and_HP\t0\t1\n",
      "001320040801BG\t0\t1\n",
      "001419991214kaminski\t0\t0\n",
      "001419991215farmer\t0\t0\n",
      "001420010212kitchen\t0\t0\n",
      "001420010704SA_and_HP\t0\t1\n",
      "001420031219GP\t0\t1\n",
      "001420040801BG\t0\t1\n",
      "001519991214kaminski\t0\t0\n",
      "001519991215farmer\t0\t0\n",
      "001520000609lokay\t0\t0\n",
      "001520010212kitchen\t0\t0\n",
      "001520010705SA_and_HP\t0\t1\n",
      "001520031219GP\t0\t1\n",
      "001619991215farmer\t0\t0\n",
      "001620010212kitchen\t0\t0\n",
      "001620010705SA_and_HP\t0\t1\n",
      "001620010706SA_and_HP\t0\t1\n",
      "001620031219GP\t0\t1\n",
      "001620040801BG\t0\t1\n",
      "001719991214kaminski\t0\t0\n",
      "001720000117beck\t0\t0\n",
      "001720010403williams\t0\t0\n",
      "001720031218GP\t0\t1\n",
      "001720040801BG\t0\t1\n",
      "001720040802BG\t0\t1\n",
      "001819991214kaminski\t0\t0\n",
      "001820010713SA_and_HP\t1\t1\n",
      "001820031218GP\t1\t1\n",
      "Accuracy:  58 %\n"
     ]
    }
   ],
   "source": [
    "!./pNaiveBayes.sh 4 assistance\n",
    "!cat *.output\n",
    "!cat *.output | awk -F'\\t' '{if ($2==$3) {corrects+=1;} total+=1; \\\n",
    "                            }END{ print \"Accuracy: \",corrects*100/total,\"%\" }'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW1.4\n",
    "Provide a mapper/reducer pair that, when executed by pNaiveBayes.sh\n",
    "   will classify the email messages by a list of one or more user-specified words. Examine the words “assistance”, “valium”, and “enlargementWithATypo” and report your results\n",
    "   \n",
    "To do so, make sure that\n",
    "\n",
    "   - mapper.py counts all occurrences of a list of words, and\n",
    "   - reducer.py performs the multiple-word multinomial Naive Bayes classification via the chosen list.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/env python\n",
    "import re, sys\n",
    "filename = sys.argv[1]\n",
    "findwords = sys.argv[2].split()\n",
    "WORDS = re.compile(r'[\\w]+')\n",
    "for line in open(filename,'r').readlines():\n",
    "    \n",
    "    ## Read lines from data chunk ##\n",
    "    # Remove non-word, non-whitespace characters\n",
    "    line = re.sub(r'[^\\w\\s\\t]+','',line.strip())\n",
    "    components = line.split('\\t')\n",
    "    \n",
    "    ## Only process valid records ##\n",
    "    try:\n",
    "        spamdoc = int(components[1])\n",
    "    except IndexError:\n",
    "        continue \n",
    "    \n",
    "    ## Compile list of words ##\n",
    "    words = ' '.join(components[2:])\n",
    "    wordslist = WORDS.findall(words)\n",
    "    \n",
    "    ## Count total words in document ##\n",
    "    totalwords = len(wordslist)\n",
    "    \n",
    "    ## Find words based on user input ##\n",
    "    for word in findwords:\n",
    "        word_cnt = len([ w for w in wordslist if w==word ])\n",
    "        # Send results for each inputted word to reducer\n",
    "        print('{}\\t{}\\t{}\\t{}'.format(components[0],word,word_cnt,spamdoc))\n",
    "    \n",
    "    ## Send totals to reducer ##\n",
    "    print('{}\\t{}\\t{}\\t{}'.format(components[0],'allwords',totalwords,spamdoc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod +x mapper.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/env python\n",
    "import sys\n",
    "import math\n",
    "filenames = sys.argv[1:]\n",
    "docs = 0\n",
    "\n",
    "typecnt = {}\n",
    "prior = {}\n",
    "findwords = {}\n",
    "wordcnt = {}\n",
    "totalwords = {}\n",
    "\n",
    "doccnts = {}\n",
    "\n",
    "for f in filenames:\n",
    "    for line in open(f,'r').readlines():\n",
    "        \n",
    "        ## Read in lines from Mapper ##\n",
    "        line = line.strip()\n",
    "        components = line.split('\\t')\n",
    "        cid,word,word_cnt,spam = components\n",
    "        \n",
    "        ## Count of words matching user input ## \n",
    "        if word!='allwords':\n",
    "            if spam not in findwords: \n",
    "                wordcnt[word] = int(word_cnt)\n",
    "                findwords[spam] = wordcnt\n",
    "            else: \n",
    "                if word not in wordcnt:\n",
    "                    wordcnt[word] = int(word_cnt)\n",
    "                else:\n",
    "                    wordcnt[word] += int(word_cnt)\n",
    "                findwords[spam] = wordcnt\n",
    "\n",
    "            # Store counts for the current document \n",
    "            if word not in doccnts:\n",
    "                doccnts[word] = int(word_cnt)\n",
    "            else:\n",
    "                doccnts[word] += int(word_cnt)\n",
    "        \n",
    "        ## Handle end of document ##\n",
    "        else:\n",
    "            # Total documents\n",
    "            docs += 1\n",
    "        \n",
    "            # Count of class \n",
    "            if spam not in typecnt: typecnt[spam] = 1\n",
    "            else: typecnt[spam] += 1\n",
    "            \n",
    "            # Count of all words \n",
    "            if spam not in totalwords: totalwords[spam] = int(word_cnt)\n",
    "            else: totalwords[spam] += int(word_cnt)\n",
    "        \n",
    "            ## Make prediction based on data available at time of first pass ##\n",
    "            # Recalculate probability given each inputted word\n",
    "            logp = 0\n",
    "            try:\n",
    "                logprior = math.log((typecnt['1']*1.0 / docs)/(typecnt['0']*1.0 / docs))\n",
    "            except KeyError:\n",
    "                logprior = 0\n",
    "            for w in doccnts:\n",
    "                if w!='allwords':\n",
    "                    try:\n",
    "                        spamp = (findwords['1'][w]*1.0/totalwords['1'])**doccnts[w]\n",
    "                    except KeyError:\n",
    "                        spamp = 0\n",
    "                    nonspamp = (findwords['0'][w]*1.0/totalwords['0'])**doccnts[w]\n",
    "                    logp += math.log(spamp/nonspamp) if nonspamp!=0 and spamp!=0 else 0 \n",
    "            \n",
    "            doccnts = {}\n",
    "            if (logp+logprior)>0: \n",
    "                print('{}\\t{}\\t{}'.format(cid,1,spam))\n",
    "            else:\n",
    "                print('{}\\t{}\\t{}'.format(cid,0,spam))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod +x reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "000119991210farmer\t0\t0\n",
      "000119991210kaminski\t0\t0\n",
      "000120000117beck\t0\t0\n",
      "000120000606lokay\t0\t0\n",
      "000120010207kitchen\t0\t0\n",
      "000120010402williams\t0\t0\n",
      "000219991213farmer\t0\t0\n",
      "000220010207kitchen\t0\t0\n",
      "000220010525SA_and_HP\t0\t1\n",
      "000220031218GP\t0\t1\n",
      "000220040801BG\t1\t1\n",
      "000319991210kaminski\t0\t0\n",
      "000319991214farmer\t0\t0\n",
      "000320000117beck\t0\t0\n",
      "000320010208kitchen\t0\t0\n",
      "000320031218GP\t0\t1\n",
      "000320040801BG\t0\t1\n",
      "000419991210kaminski\t1\t0\n",
      "000419991214farmer\t0\t0\n",
      "000420010402williams\t0\t0\n",
      "000420010612SA_and_HP\t0\t1\n",
      "000420040801BG\t0\t1\n",
      "000519991212kaminski\t1\t0\n",
      "000519991214farmer\t0\t0\n",
      "000520000606lokay\t0\t0\n",
      "000520010208kitchen\t0\t0\n",
      "000520010623SA_and_HP\t0\t1\n",
      "000520031218GP\t0\t1\n",
      "000619991213kaminski\t0\t0\n",
      "000620010208kitchen\t0\t0\n",
      "000620010403williams\t0\t0\n",
      "000620010625SA_and_HP\t0\t1\n",
      "000620031218GP\t0\t1\n",
      "000620040801BG\t0\t1\n",
      "000719991213kaminski\t0\t0\n",
      "000719991214farmer\t0\t0\n",
      "000720000117beck\t0\t0\n",
      "000720010209kitchen\t0\t0\n",
      "000720031218GP\t0\t1\n",
      "000720040801BG\t0\t1\n",
      "000820010209kitchen\t0\t0\n",
      "000820010612SA_and_HP\t0\t1\n",
      "000820010625SA_and_HP\t0\t1\n",
      "000820031218GP\t0\t1\n",
      "000820040801BG\t0\t1\n",
      "000919991213kaminski\t0\t0\n",
      "000919991214farmer\t0\t0\n",
      "000920000607lokay\t0\t0\n",
      "000920010209kitchen\t0\t0\n",
      "000920010626SA_and_HP\t0\t1\n",
      "000920031218GP\t1\t1\n",
      "001019991214farmer\t0\t0\n",
      "001019991214kaminski\t0\t0\n",
      "001020010209kitchen\t0\t0\n",
      "001020010628SA_and_HP\t1\t1\n",
      "001020031218GP\t0\t1\n",
      "001020040801BG\t0\t1\n",
      "001119991214farmer\t0\t0\n",
      "001120010628SA_and_HP\t0\t1\n",
      "001120010629SA_and_HP\t0\t1\n",
      "001120031218GP\t0\t1\n",
      "001120040801BG\t0\t1\n",
      "001219991214farmer\t0\t0\n",
      "001219991214kaminski\t0\t0\n",
      "001220000117beck\t0\t0\n",
      "001220000608lokay\t0\t0\n",
      "001220010209kitchen\t0\t0\n",
      "001220031219GP\t0\t1\n",
      "001319991214farmer\t0\t0\n",
      "001319991214kaminski\t0\t0\n",
      "001320010403williams\t0\t0\n",
      "001320010630SA_and_HP\t0\t1\n",
      "001320040801BG\t0\t1\n",
      "001419991214kaminski\t0\t0\n",
      "001419991215farmer\t0\t0\n",
      "001420010212kitchen\t0\t0\n",
      "001420010704SA_and_HP\t0\t1\n",
      "001420031219GP\t0\t1\n",
      "001420040801BG\t0\t1\n",
      "001519991214kaminski\t0\t0\n",
      "001519991215farmer\t0\t0\n",
      "001520000609lokay\t0\t0\n",
      "001520010212kitchen\t0\t0\n",
      "001520010705SA_and_HP\t0\t1\n",
      "001520031219GP\t0\t1\n",
      "001619991215farmer\t0\t0\n",
      "001620010212kitchen\t0\t0\n",
      "001620010705SA_and_HP\t0\t1\n",
      "001620010706SA_and_HP\t0\t1\n",
      "001620031219GP\t0\t1\n",
      "001620040801BG\t0\t1\n",
      "001719991214kaminski\t0\t0\n",
      "001720000117beck\t0\t0\n",
      "001720010403williams\t0\t0\n",
      "001720031218GP\t0\t1\n",
      "001720040801BG\t0\t1\n",
      "001720040802BG\t0\t1\n",
      "001819991214kaminski\t0\t0\n",
      "001820010713SA_and_HP\t0\t1\n",
      "001820031218GP\t0\t1\n",
      "Accuracy:  57 %\n"
     ]
    }
   ],
   "source": [
    "!./pNaiveBayes.sh 4 \"assistance valium enlargement\"\n",
    "!cat *.output\n",
    "!cat *.output | awk -F'\\t' '{if ($2==$3) {corrects+=1;} total+=1; \\\n",
    "                            }END{ print \"Accuracy: \",corrects*100/total,\"%\" }'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW1.5 \n",
    "Provide a mapper/reducer pair that, when executed by pNaiveBayes.sh\n",
    "   will classify the email messages by all words present.\n",
    "   To do so, make sure that\n",
    "\n",
    "   - mapper.py counts all occurrences of all words, and\n",
    "   - reducer.py performs a word-distribution-wide Naive Bayes classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/env python\n",
    "import re, sys\n",
    "filename = sys.argv[1]\n",
    "vocab = set()\n",
    "WORDS = re.compile(r'[\\w]+')\n",
    "for line in open(filename,'r').readlines():\n",
    "    \n",
    "    ## Read lines from data chunk ##\n",
    "    # Remove non-word, non-whitespace characters\n",
    "    line = re.sub(r'[^\\w\\s\\t]+','',line.strip())\n",
    "    components = line.split('\\t')\n",
    "    \n",
    "    ## Only process valid records ##\n",
    "    try:\n",
    "        spamdoc = int(components[1])\n",
    "    except IndexError:\n",
    "        continue \n",
    "    \n",
    "    ## Compile list of words ##\n",
    "    words = ' '.join(components[2:])\n",
    "    wordslist = WORDS.findall(words)\n",
    "    vocab = vocab.union(set(wordslist))\n",
    "    \n",
    "    ## Count total words in document ##\n",
    "    totalwords = len(wordslist)\n",
    "    \n",
    "    ## Find words based on user input ##\n",
    "    for word in vocab:\n",
    "        word_cnt = len([ w for w in wordslist if w==word ])\n",
    "        # Send results for each inputted word to reducer\n",
    "        print('{}\\t{}\\t{}\\t{}'.format(components[0],word,word_cnt,spamdoc))\n",
    "    \n",
    "    ## Send totals to reducer ##\n",
    "    print('{}\\t{}\\t{}\\t{}'.format(components[0],'allwords',totalwords,spamdoc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod +x mapper.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/env python\n",
    "import sys\n",
    "import math\n",
    "filenames = sys.argv[1:]\n",
    "docs = 0\n",
    "\n",
    "typecnt = {}\n",
    "prior = {}\n",
    "findwords = {}\n",
    "wordcnt = {}\n",
    "totalwords = {}\n",
    "\n",
    "for f in filenames:\n",
    "    for line in open(f,'r').readlines():\n",
    "        \n",
    "        ## Read in lines from Mapper ##\n",
    "        line = line.strip()\n",
    "        components = line.split('\\t')\n",
    "        cid,word,word_cnt,spam = components\n",
    "        \n",
    "        if word!='allwords':\n",
    "            ## Count of words matching user input ## \n",
    "            if spam not in findwords: \n",
    "                wordcnt[word] = int(word_cnt)\n",
    "                findwords[spam] = wordcnt\n",
    "            else: \n",
    "                if word not in wordcnt:\n",
    "                    wordcnt[word] = int(word_cnt)\n",
    "                else:\n",
    "                    wordcnt[word] += int(word_cnt)\n",
    "                findwords[spam] = wordcnt\n",
    "        \n",
    "        else:\n",
    "            # Total documents\n",
    "            docs += 1\n",
    "            \n",
    "            # Count of class \n",
    "            if spam not in typecnt: typecnt[spam] = 1\n",
    "            else: typecnt[spam] += 1\n",
    "                \n",
    "            # Count of all words \n",
    "            if spam not in totalwords: totalwords[spam] = int(word_cnt)\n",
    "            else: totalwords[spam] += int(word_cnt)\n",
    "\n",
    "doccnts = {}\n",
    "## Make a second pass through the data ##\n",
    "for f in filenames:\n",
    "    for line in open(f,'r').readlines():\n",
    "        \n",
    "        ## Read in lines from Mapper ##\n",
    "        line = line.strip()\n",
    "        components = line.split('\\t')\n",
    "        cid,word,word_cnt,spam = components\n",
    "        \n",
    "        if word not in doccnts:\n",
    "            doccnts[word] = int(word_cnt)\n",
    "        else:\n",
    "            doccnts[word] += int(word_cnt)\n",
    "        \n",
    "        if word=='allwords':\n",
    "            # Recalculate probability given each inputted word\n",
    "            spamp = 0\n",
    "            nonspamp = 0\n",
    "            log_sp = 0\n",
    "            \n",
    "            prior_s = (typecnt['1']*1.0) / docs\n",
    "            prior_p = (typecnt['0']*1.0) / docs\n",
    "            log_prior = math.log(prior_s/prior_p)\n",
    "\n",
    "            for w in doccnts:\n",
    "                if w!='allwords':\n",
    "                    spamp = ((findwords['1'][w]*1.0) / (totalwords['1']))**doccnts[w]\n",
    "                    nonspamp = ((findwords['0'][w]*1.0) / (totalwords['0']))**doccnts[w]\n",
    "                    log_sp += math.log(spamp/nonspamp)\n",
    "            \n",
    "            doccnts = {}\n",
    "            \n",
    "            if (log_sp+log_prior)>0: \n",
    "                print('{}\\t{}\\t{}'.format(cid,1,spam))\n",
    "            else:\n",
    "                print('{}\\t{}\\t{}'.format(cid,0,spam))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod +x reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "000119991210farmer\t0\t0\n",
      "000119991210kaminski\t0\t0\n",
      "000120000117beck\t0\t0\n",
      "000120000606lokay\t0\t0\n",
      "000120010207kitchen\t0\t0\n",
      "000120010402williams\t0\t0\n",
      "000219991213farmer\t0\t0\n",
      "000220010207kitchen\t0\t0\n",
      "000220010525SA_and_HP\t0\t1\n",
      "000220031218GP\t0\t1\n",
      "000220040801BG\t0\t1\n",
      "000319991210kaminski\t0\t0\n",
      "000319991214farmer\t0\t0\n",
      "000320000117beck\t0\t0\n",
      "000320010208kitchen\t0\t0\n",
      "000320031218GP\t0\t1\n",
      "000320040801BG\t0\t1\n",
      "000419991210kaminski\t0\t0\n",
      "000419991214farmer\t0\t0\n",
      "000420010402williams\t0\t0\n",
      "000420010612SA_and_HP\t0\t1\n",
      "000420040801BG\t0\t1\n",
      "000519991212kaminski\t0\t0\n",
      "000519991214farmer\t0\t0\n",
      "000520000606lokay\t0\t0\n",
      "000520010208kitchen\t0\t0\n",
      "000520010623SA_and_HP\t0\t1\n",
      "000520031218GP\t0\t1\n",
      "000619991213kaminski\t0\t0\n",
      "000620010208kitchen\t0\t0\n",
      "000620010403williams\t0\t0\n",
      "000620010625SA_and_HP\t0\t1\n",
      "000620031218GP\t0\t1\n",
      "000620040801BG\t0\t1\n",
      "000719991213kaminski\t0\t0\n",
      "000719991214farmer\t0\t0\n",
      "000720000117beck\t0\t0\n",
      "000720010209kitchen\t0\t0\n",
      "000720031218GP\t0\t1\n",
      "000720040801BG\t0\t1\n",
      "000820010209kitchen\t0\t0\n",
      "000820010612SA_and_HP\t0\t1\n",
      "000820010625SA_and_HP\t0\t1\n",
      "000820031218GP\t0\t1\n",
      "000820040801BG\t0\t1\n",
      "000919991213kaminski\t0\t0\n",
      "000919991214farmer\t0\t0\n",
      "000920000607lokay\t0\t0\n",
      "000920010209kitchen\t0\t0\n",
      "000920010626SA_and_HP\t0\t1\n",
      "000920031218GP\t0\t1\n",
      "001019991214farmer\t0\t0\n",
      "001019991214kaminski\t0\t0\n",
      "001020010209kitchen\t0\t0\n",
      "001020010628SA_and_HP\t0\t1\n",
      "001020031218GP\t0\t1\n",
      "001020040801BG\t0\t1\n",
      "001119991214farmer\t0\t0\n",
      "001120010628SA_and_HP\t0\t1\n",
      "001120010629SA_and_HP\t0\t1\n",
      "001120031218GP\t0\t1\n",
      "001120040801BG\t0\t1\n",
      "001219991214farmer\t0\t0\n",
      "001219991214kaminski\t0\t0\n",
      "001220000117beck\t0\t0\n",
      "001220000608lokay\t0\t0\n",
      "001220010209kitchen\t0\t0\n",
      "001220031219GP\t0\t1\n",
      "001319991214farmer\t0\t0\n",
      "001319991214kaminski\t0\t0\n",
      "001320010403williams\t0\t0\n",
      "001320010630SA_and_HP\t0\t1\n",
      "001320040801BG\t0\t1\n",
      "001419991214kaminski\t0\t0\n",
      "001419991215farmer\t0\t0\n",
      "001420010212kitchen\t0\t0\n",
      "001420010704SA_and_HP\t0\t1\n",
      "001420031219GP\t0\t1\n",
      "001420040801BG\t0\t1\n",
      "001519991214kaminski\t0\t0\n",
      "001519991215farmer\t0\t0\n",
      "001520000609lokay\t0\t0\n",
      "001520010212kitchen\t0\t0\n",
      "001520010705SA_and_HP\t0\t1\n",
      "001520031219GP\t0\t1\n",
      "001619991215farmer\t0\t0\n",
      "001620010212kitchen\t0\t0\n",
      "001620010705SA_and_HP\t0\t1\n",
      "001620010706SA_and_HP\t0\t1\n",
      "001620031219GP\t0\t1\n",
      "001620040801BG\t0\t1\n",
      "001719991214kaminski\t0\t0\n",
      "001720000117beck\t0\t0\n",
      "001720010403williams\t0\t0\n",
      "001720031218GP\t0\t1\n",
      "001720040801BG\t0\t1\n",
      "001720040802BG\t0\t1\n",
      "001819991214kaminski\t0\t0\n",
      "001820010713SA_and_HP\t0\t1\n",
      "001820031218GP\t0\t1\n",
      "Accuracy:  56 %\n"
     ]
    }
   ],
   "source": [
    "!./pNaiveBayes.sh 4\n",
    "!cat *.output\n",
    "!cat *.output | awk -F'\\t' '{if ($2==$3) {corrects+=1;} total+=1; \\\n",
    "                            }END{ print \"Accuracy: \",corrects*100/total,\"%\" }'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
