{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">### HW6.0. \n",
    ">In mathematics, computer science, economics, or management science what is mathematical optimization?  \n",
    "Give an example of a optimization problem that you have worked with directly or that your organization has worked on.  \n",
    "Please describe the objective function and the decision variables.  \n",
    "Was the project successful (deployed in the real world)?  \n",
    "Describe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">### HW6.1 \n",
    ">Optimization theory:  \n",
    "For unconstrained univariate optimization what are the first order  Necessary Conditions for Optimality (FOC).  \n",
    "What are the second order optimality conditions (SOC)?  \n",
    "Give a mathematical defintion.  \n",
    "Also in python, plot the univariate function \n",
    "$X^3 -12x^2-6$ defined over the real  domain -6 to +6. \n",
    "\n",
    ">Also plot its corresponding first and second derivative functions. Eyeballing these graphs, identify candidate optimal points and then classify them as local minimums or maximums.  \n",
    "Highlight and label these points in your graphs. Justify your responses using the FOC and SOC.\n",
    "\n",
    ">For unconstrained multi-variate optimization what are the first order  Necessary Conditions for Optimality (FOC).  \n",
    "What are the second order optimality conditions (SOC)?  \n",
    "Give a mathematical defintion.  \n",
    "What is the Hessian matrix in this context?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">###HW6.3 Convex optimization \n",
    ">What makes an optimization problem convex?  \n",
    "What are the first order  Necessary Conditions for Optimality in convex optimization.  \n",
    "What are the second order optimality conditions for convex optimization?  \n",
    "Are both necessary to determine the maximum or minimum of candidate optimal solutions?\n",
    "\n",
    ">Fill in the BLANKS here:  \n",
    "Convex minimization, a subfield of optimization, studies the problem of minimizing BLANK functions over BLANK sets. The BLANK property can make optimization in some sense \"easier\" than the general case - for example, any local minimum must be a global minimum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">###HW 6.4\n",
    ">The learning objective function for weighted ordinary least squares (WOLS) (aka weight linear regression) is defined as follows:  \n",
    "\n",
    ">$0.5* sumOverTrainingExample_i*(weight_i * (W * X_i - y_i)^2)$\n",
    "\n",
    ">Where training set consists of input variables X ( in vector form) and a target variable y, and W is the vector of coefficients for the linear regression model.\n",
    "\n",
    ">Derive the gradient for this weighted OLS by hand; showing each step and also explaining each step.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong>Step 1:</strong> $1/2*\\sum{ weight_i*(WX_i-y_i)^2)}$\n",
    "> This is the starting step.  \n",
    "\n",
    "<strong>Step 2:</strong> $1/2*\\sum{ weight_i*(WX_i-y_i)*(WX_i-y_i)}$\n",
    "> Square the values within the sum prior to taking derivative with respect to W  \n",
    "\n",
    "<strong>Step 3:</strong> $1/2*\\sum{ weight_i*(W^2X_i^2-2WX_iy_i+y_i^2)}$\n",
    "> Multiply terms and combine  \n",
    "\n",
    "<strong>Step 4:</strong> $1/2*\\sum{ weight_i*(2WX_i^2-2X_iy)}$\n",
    "> Take derivative with respect to W\n",
    "\n",
    "<strong>Step 5:</strong> $1/2*\\sum{ weight_i*2X_i(WX_i-y)}$\n",
    "> Factor remaining terms\n",
    "\n",
    "<strong>Step 6:</strong> $\\sum{ weight_i*X_i(WX_i-y)}$\n",
    "> 1/2 and 2 cancel out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">###HW 6.5\n",
    ">Write a MapReduce job in MRJob to do the training at scale of a weighted OLS model using gradient descent.  \n",
    "\n",
    ">Generate one million datapoints just like in the following notebook:  http://nbviewer.ipython.org/urls/dl.dropbox.com/s/kritdm3mo1daolj/MrJobLinearRegressionGD.ipynb  \n",
    "\n",
    ">Weight each example as follows: \n",
    "\n",
    ">$weight(x)= abs(1/x)$\n",
    "\n",
    ">Sample 1% of the data in MapReduce and use the sampled dataset to train a (weighted if available in SciKit-Learn) linear regression model locally using  SciKit-Learn (http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html)\n",
    "\n",
    ">Plot the resulting weighted linear regression model versus the original model that you used to generate the data. Comment on your findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pylab \n",
    "size = 1000000\n",
    "x = np.random.uniform(-4, 4, size)\n",
    "y = x * 1.0 - 4 + np.random.normal(0,0.5,size)\n",
    "data = zip(y,x)\n",
    "np.savetxt('LinearRegression.csv',data, delimiter = \",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-6.650992125999841242e+00,-2.625148229118576815e+00\r\n",
      "-1.798573558430904828e+00,1.501572938676488000e+00\r\n",
      "-4.738800795810907296e+00,-4.718581851287790840e-01\r\n",
      "-4.564200222550757857e+00,-1.741144385132784578e-01\r\n",
      "-6.169588744364752131e+00,-1.597100336185655500e+00\r\n",
      "-4.927822157877639775e+00,-5.294701138536934693e-01\r\n",
      "-6.149374572233639036e+00,-1.520726945757780335e+00\r\n",
      "-8.180457655822662488e+00,-3.847785247801682296e+00\r\n",
      "-1.230371438409911145e+00,2.830359334988319375e+00\r\n",
      "-4.483833424271095325e+00,1.563974898459772334e-01\r\n"
     ]
    }
   ],
   "source": [
    "!head LinearRegression.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%writefile GD_WOLS_LinearRegression.py\n",
    "import numpy as np\n",
    "from mrjob.job import MRJob\n",
    "class GD_WOLS_LinearRegression(MRJob):\n",
    "    INTERNAL_PROTOCOL = PickleProtocol\n",
    "    \n",
    "    def configure_options(self):\n",
    "        super(GD_WOLS_LinearRegression, self).configure_options()\n",
    "        self.add_passthrough_option(\n",
    "            '--weights_file'\n",
    "                , dest='weights_file'\n",
    "                , help='Weights file')\n",
    "    \n",
    "    def init_weights(self):\n",
    "        # check weights file \n",
    "        with open(self.options.weights_file,'r') as r:\n",
    "            self.weights = np.array(float(v) for v in r.readline().split(','))\n",
    "        \n",
    "        # initialze gradient for this iteration\n",
    "        self.partial_gradient_values = np.array([0]*len(self.weights))\n",
    "        self.partial_count = 0\n",
    "    \n",
    "    def partial_gradient(self, _, line):\n",
    "        D = map(float,line.split(','))\n",
    "        y_hat = self.weights[0]+self.weights[1:]*D[1:]\n",
    "        self.partial_gradient_values = np.array(self.partial_gradient_values[0]\n",
    "                                            + D[0]-y_hat\n",
    "                                         , self.partial_gradient_values[1]\n",
    "                                             +(D[0]-y_hat)*D[1])\n",
    "        self.partial_count = self.partial_count + 1\n",
    "    \n",
    "    def partial_gradient_emit(self):\n",
    "        yield None, (self.partial_gradient_values, self.partial_count)\n",
    "    \n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">###HW6.6 Clean up notebook for GMM via EM\n",
    "\n",
    ">Using the following notebook as a starting point:\n",
    "\n",
    ">http://nbviewer.jupyter.org/urls/dl.dropbox.com/s/0t7985e40fovlkw/EM-GMM-MapReduce%20Design%201.ipynb \n",
    "\n",
    ">Improve this notebook as follows:  \n",
    "* Add in equations into the notebook (not images of equations)   \n",
    "* Number the equations  \n",
    "* Make sure the equation notation matches the code and the code and comments refer to the equations numbers  \n",
    "* Comment the code  \n",
    "* Rename/Reorganize the code to make it more readable  \n",
    "* Rerun the examples similar graphics (or possibly better graphics)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">### HW6.7 Implement Bernoulli Mixture Model via EM\n",
    ">Implement the EM clustering algorithm to determine Bernoulli Mixture Model for discrete data in MRJob.\n",
    "\n",
    ">As a unit test:\n",
    "\n",
    "\n",
    ">As a test: use the same dataset from HW 4.5, the Tweet Dataset. \n",
    "Using this data, you will implement a 1000-dimensional EM-based Bernoulli Mixture Model  algorithm in MrJob on the users\n",
    "by their 1000-dimensional word stripes/vectors using K = 4.  \n",
    "Repeat this experiment using your KMeans MRJob implementation fron HW4.  \n",
    "Report the rand index score using the class code as ground truth label for both algorithms and comment on your findings.\n",
    "\n",
    ">Here is some more information on the Tweet Dataset.\n",
    "\n",
    ">Here you will use a different dataset consisting of word-frequency distributions \n",
    "for 1,000 Twitter users. These Twitter users use language in very different ways,\n",
    "and were classified by hand according to the criteria:\n",
    "\n",
    ">0: Human, where only basic human-human communication is observed.\n",
    "\n",
    ">1: Cyborg, where language is primarily borrowed from other sources\n",
    "(e.g., jobs listings, classifieds postings, advertisements, etc...).\n",
    "\n",
    ">2: Robot, where language is formulaically derived from unrelated sources\n",
    "(e.g., weather/seismology, police/fire event logs, etc...).\n",
    "\n",
    ">3: Spammer, where language is replicated to high multiplicity\n",
    "(e.g., celebrity obsessions, personal promotion, etc... )\n",
    "\n",
    ">Check out the preprints of  recent research,\n",
    "which spawned this dataset:\n",
    "\n",
    ">http://arxiv.org/abs/1505.04342  \n",
    ">http://arxiv.org/abs/1508.01843\n",
    "\n",
    ">The main data lie in the accompanying file:\n",
    "\n",
    ">topUsers_Apr-Jul_2014_1000-words.txt\n",
    "\n",
    ">and are of the form:\n",
    "\n",
    ">USERID,CODE,TOTAL,WORD1_COUNT,WORD2_COUNT,...\n",
    ">.\n",
    ">.\n",
    "\n",
    ">where\n",
    "\n",
    ">USERID = unique user identifier\n",
    ">CODE = 0/1/2/3 class code\n",
    ">TOTAL = sum of the word counts\n",
    "\n",
    ">Using this data, you will implement a 1000-dimensional K-means algorithm in MrJob on the users\n",
    "by their 1000-dimensional word stripes/vectors using several \n",
    "centroid initializations and values of K."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
