{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ET:13 Given the following paired RDDs   \n",
    "RDD1 = {(1, 2), (3, 4), (3, 6)}  \n",
    "RDD2 = {(3, 9) (3, 6)}  \n",
    "\n",
    "Using PySpark, write code to perform an inner join of these paired RDDs. What is the resulting RDD? Make your Spark available in your notebook:  \n",
    "\n",
    "A: [(3, (4, 9)), (3, (6, 9))]  \n",
    "B: [(3, (4, 9)), (3, (4, 6)), (3, (6, 9)), (3, (6, 6))]  \n",
    "C: [(3, (4, 9)), (3, (4, 6)), (3, (6, 9)), (3, (6, 9))]  \n",
    "D: None of the above  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to\n",
      "      ____              __\n",
      "     / __/__  ___ _____/ /__\n",
      "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
      "   /__ / .__/\\_,_/_/ /_/\\_\\   version 1.6.1\n",
      "      /_/\n",
      "\n",
      "Using Python version 2.7.10 (default, Oct 23 2015 19:19:21)\n",
      "SparkContext available as sc, SQLContext available as sqlContext.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import sys #current as of 9/26/2015\n",
    "spark_home = os.environ['SPARK_HOME'] = \\\n",
    "   '/opt/spark16'\n",
    "    \n",
    "sys.path.insert(0,os.path.join(spark_home,'python'))\n",
    "sys.path.insert(0,os.path.join(spark_home,'python/lib/py4j-0.8.2.1-src.zip'))\n",
    "execfile(os.path.join(spark_home,'python/pyspark/shell.py'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(3, (4, 9)), (3, (4, 6)), (3, (6, 9)), (3, (6, 6))]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RDD1 = sc.parallelize([(1, 2), (3, 4), (3, 6)])\n",
    "RDD2 = sc.parallelize([(3, 9), (3, 6)])\n",
    "RDD1.join(RDD2).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ET:15 \n",
    "Use Spark and the following notebook, https://www.dropbox.com/s/6s5ph41h74bggwi/Linear-Regression-on-Beer-Data.ipynb?dl=0 to answer this question.\n",
    "\n",
    "The mean absolute percentage error (MAPE), also known as mean absolute percentage deviation (MAPD), is a measure of prediction accuracy of a model for say a forecasting method in statistics, \n",
    "for example in trend estimation. It usually expresses accuracy as a percentage, and is defined by the formula:\n",
    "\n",
    "MAPE = average over all examples (100*Abs(Actual - Predicted) / Actual)) \n",
    "\n",
    "Note when Actual is zero that test row is dropped from the evaluation.\n",
    "\n",
    "Construct a mean model for target variable CASES18PK. Calculate the MAPE for the mean model over the training set. Select the closest answer.\n",
    "\n",
    "(a) 200%\n",
    "(b) 250%\n",
    "(c) 20%\n",
    "(d) 180%\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df=sc.textFile(\"beerSales.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nums= df.map(lambda x: x.split(\"\\t\")[2]).\\\n",
    "        filter(lambda x: '.' in x).\\\n",
    "        map(lambda x: float(x))\n",
    "mean=  nums.reduce(lambda x,y: x+y) / nums.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16.724615384615387"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "MAPE = lambda x, p: 100*abs(x-p)/x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14.581300722435568"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nums.map(lambda x: MAPE(x, mean)).reduce(lambda x,y: x+y) / nums.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ET:16\n",
    "Use Spark and the following notebook, https://www.dropbox.com/s/6s5ph41h74bggwi/Linear-Regression-on-Beer-Data.ipynb?dl=0 to answer this question.  \n",
    "\n",
    "The target variable CASES18PK is skewed, so take the log of it (and make it more normally distributed) and compute the MAPE of the mean model for CASES18PK\n",
    "Select the closest answer to your calculated MAPE.  \n",
    "\n",
    "(a) 200%  \n",
    "(b) 30%  \n",
    "(c) 20%  \n",
    "(d) 10%  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from math import log\n",
    "nums_log = nums.map(lambda x: log(x))\n",
    "mean=  nums_log.reduce(lambda x,y: x+y) / nums_log.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.80630886928261"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.165527753854489"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nums_log.map(lambda x: MAPE(x, mean)).reduce(lambda x,y: x+y) / nums_log.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ET:17\n",
    "Use Spark and the following notebook, https://www.dropbox.com/s/6s5ph41h74bggwi/Linear-Regression-on-Beer-Data.ipynb?dl=0 to answer this question.  \n",
    "\n",
    "Build a linear regression model using the following variables:  \n",
    "\n",
    "Log(CASES18PK)  ~  log(PRICE12PK), \tlog(PRICE18PK),\tlog(PRICE30PK)  \n",
    "\n",
    "Calculate MAPE over the test set and select the closest answer.  \n",
    "\n",
    "(a) 4.3%  \n",
    "(b) 4.6%  \n",
    "(c) 3.5%  \n",
    "(d) 3.9%  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.regression import LinearRegressionWithSGD, LabeledPoint\n",
    "import numpy as np\n",
    "\n",
    "def mapPoints(x):\n",
    "    _, PRICE12PK, \\\n",
    "        PRICE18PK, PRICE30PK, \\\n",
    "        CASES12PK, CASES18PK, \\\n",
    "        CASES30PK                = x.split(\"\\t\")\n",
    "    dvars = map(float,[PRICE12PK, PRICE18PK, PRICE30PK])\n",
    "    return LabeledPoint(log(float(CASES18PK)), map(log,dvars))\n",
    "    \n",
    "df_points= df.filter(lambda x: '.' in x)\\\n",
    "        .map(lambda x: mapPoints(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = LinearRegressionWithSGD.train(df_points\\\n",
    "                                      , 1000\\\n",
    "                                      , step=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "non_labeled = df_points.map(lambda x: x.features)\n",
    "labels = df_points.map(lambda x: x.label)\n",
    "preds = model.predict(non_labeled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20.008479360530085"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.zip(preds).map(lambda x: MAPE(x[0],x[1])).reduce(lambda x,y: x+y) / preds.count()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
