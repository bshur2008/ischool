{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Kaggle -- Bimbo\n",
    "Demand prediction for Mexican food company Bimbo stores+clients+products. Will use Spark because training data is somewhat sizable (>3GB uncompressed) and there are quite a lot of combinations for the different Agencies, Channels, Routes, Clients, and Products. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load functions and args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import hashlib\n",
    "import numpy as np\n",
    "from math import log, sqrt\n",
    "from collections import defaultdict\n",
    "from pyspark.mllib.linalg import SparseVector\n",
    "from pyspark.mllib.regression import LabeledPoint, LinearRegressionWithSGD\n",
    "\n",
    "\n",
    "def parsePoint(point, start_i, end_i, sep=','):\n",
    "    \"\"\"Converts a comma separated string into a list of (featureID, value) tuples.\n",
    "\n",
    "    Note:\n",
    "        featureIDs should start at 0 and increase to the number of features - 1.\n",
    "\n",
    "    Args:\n",
    "        point (str): A comma separated string where the first value is the label and the rest\n",
    "            are features.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of (featureID, value) tuples.\n",
    "    \"\"\"\n",
    "    fields = [ (i,v) for i,v in enumerate(point.split(sep)[start_i:end_i]) ]\n",
    "    return fields\n",
    "\n",
    "\n",
    "def createOneHotDict(inputData):\n",
    "    \"\"\"Creates a one-hot-encoder dictionary based on the input data.\n",
    "\n",
    "    Args:\n",
    "        inputData (RDD of lists of (int, str)): An RDD of observations where each observation is\n",
    "            made up of a list of (featureID, value) tuples.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary where the keys are (featureID, value) tuples and map to values that are\n",
    "            unique integers.\n",
    "    \"\"\"\n",
    "    distinctFeatures = inputData.flatMap(lambda x: x).distinct()\n",
    "    outputDict = distinctFeatures.zipWithIndex().collectAsMap()\n",
    "    return outputDict\n",
    "\n",
    "\n",
    "def oneHotEncoding(rawFeats, OHEDict, numOHEFeats):\n",
    "    \"\"\"Produce a one-hot-encoding from a list of features and an OHE dictionary.\n",
    "\n",
    "    Note:\n",
    "        If a (featureID, value) tuple doesn't have a corresponding key in OHEDict it should be\n",
    "        ignored.\n",
    "\n",
    "    Args:\n",
    "        rawFeats (list of (int, str)): The features corresponding to a single observation.  Each\n",
    "            feature consists of a tuple of featureID and the feature's value. (e.g. sampleOne)\n",
    "        OHEDict (dict): A mapping of (featureID, value) to unique integer.\n",
    "        numOHEFeats (int): The total number of unique OHE features (combinations of featureID and\n",
    "            value).\n",
    "\n",
    "    Returns:\n",
    "        SparseVector: A SparseVector of length numOHEFeats with indicies equal to the unique\n",
    "            identifiers for the (featureID, value) combinations that occur in the observation and\n",
    "            with values equal to 1.0.\n",
    "    \"\"\"\n",
    "    ref = {}\n",
    "    for k in rawFeats:\n",
    "        if k in OHEDict:\n",
    "            ref.update({OHEDict[k]:1})\n",
    "    return SparseVector(numOHEFeats, ref)\n",
    "\n",
    "\n",
    "def parseOHEPoint(point, OHEDict, numOHEFeats,  start_i=1, end_i=-5, sep=',', train=True):\n",
    "    \"\"\"Obtain the label and feature vector for this raw observation.\n",
    "\n",
    "    Note:\n",
    "        You must use the function `oneHotEncoding` in this implementation or later portions\n",
    "        of this lab may not function as expected.\n",
    "\n",
    "    Args:\n",
    "        point (str): A comma separated string where the first value is the label and the rest\n",
    "            are features.\n",
    "        OHEDict (dict of (int, str) to int): Mapping of (featureID, value) to unique integer.\n",
    "        numOHEFeats (int): The number of unique features in the training dataset.\n",
    "\n",
    "    Returns:\n",
    "        LabeledPoint: Contains the label for the observation and the one-hot-encoding of the\n",
    "            raw features based on the provided OHE dictionary.\n",
    "    \"\"\"\n",
    "    splits = point.split(sep)\n",
    "    fields = [ (i,v) for i,v in enumerate(splits[start_i:end_i]) ]\n",
    "    if train:\n",
    "        label = splits[-1] if splits[-1]>0 else 0\n",
    "        lp = LabeledPoint(label, oneHotEncoding(fields, OHEDict, numOHEFeats))\n",
    "    else:\n",
    "        lp = oneHotEncoding(fields, OHEDict, numOHEFeats)\n",
    "    return lp\n",
    "\n",
    "\n",
    "def hashFunction(numBuckets, rawFeats, printMapping=False):\n",
    "    \"\"\"Calculate a feature dictionary for an observation's features based on hashing.\n",
    "\n",
    "    Note:\n",
    "        Use printMapping=True for debug purposes and to better understand how the hashing works.\n",
    "\n",
    "    Args:\n",
    "        numBuckets (int): Number of buckets to use as features.\n",
    "        rawFeats (list of (int, str)): A list of features for an observation.  Represented as\n",
    "            (featureID, value) tuples.\n",
    "        printMapping (bool, optional): If true, the mappings of featureString to index will be\n",
    "            printed.\n",
    "\n",
    "    Returns:\n",
    "        dict of int to float:  The keys will be integers which represent the buckets that the\n",
    "            features have been hashed to.  The value for a given key will contain the count of the\n",
    "            (featureID, value) tuples that have hashed to that key.\n",
    "    \"\"\"\n",
    "    mapping = {}\n",
    "    for ind, category in rawFeats:\n",
    "        featureString = category + str(ind)\n",
    "        mapping[featureString] = int(int(hashlib.md5(featureString).hexdigest(), 16) % numBuckets)\n",
    "    if(printMapping): print mapping\n",
    "    sparseFeatures = defaultdict(float)\n",
    "    for bucket in mapping.values():\n",
    "        sparseFeatures[bucket] += 1.0\n",
    "    return dict(sparseFeatures)\n",
    "\n",
    "\n",
    "def parseHashPoint(point, numBuckets, start_i=1, end_i=-5, sep=',', train=True):\n",
    "    \"\"\"Create a LabeledPoint for this observation using hashing.\n",
    "\n",
    "    Args:\n",
    "        point (str): A comma separated string where the first value is the label and the rest are\n",
    "            features.\n",
    "        numBuckets: The number of buckets to hash to.\n",
    "        sep: The separator of the input features.\n",
    "\n",
    "    Returns:\n",
    "        LabeledPoint: A LabeledPoint with a label (0.0 or 1.0) and a SparseVector of hashed\n",
    "            features.\n",
    "    \"\"\"\n",
    "    data = point.split(sep)\n",
    "    features = data[start_i:end_i]\n",
    "    indexed_features = [(i, feature) for i, feature in enumerate(features)]\n",
    "    if train:\n",
    "        label = data[-1] if data[-1]>0 else 0\n",
    "        return LabeledPoint(label, SparseVector(numBuckets, hashFunction(numBuckets, indexed_features)))\n",
    "    else:\n",
    "        return SparseVector(numBuckets, hashFunction(numBuckets, indexed_features))\n",
    "\n",
    "\n",
    "def predict(x, weights, intercept):\n",
    "    ''' Make a prediction \n",
    "        from x \n",
    "        based on model weights\n",
    "    '''\n",
    "    prediction = x.features.dot(weights) + intercept\n",
    "    return (prediction, x.label)\n",
    "\n",
    "\n",
    "def rmsle(preds):\n",
    "    ''' With predictions\n",
    "        output RMSLE\n",
    "    '''\n",
    "    N = preds.count()\n",
    "    pred_log_avg_sq_error = preds.map(\n",
    "        lambda (p,x): (log(p+1)-log(x+1))**2\n",
    "    ).reduce( \n",
    "        lambda x,y: x+y\n",
    "    ) / N\n",
    "    return sqrt(pred_log_avg_sq_error)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### declare args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DATA_DIR = '/Data/'\n",
    "TRAIN_FILE = 'train.csv.gz'\n",
    "TEST_FILE = 'test.csv.gz'\n",
    "PRODUCT_FILE = 'producto_tabla.csv.gz'\n",
    "CLIENT_FILE = 'cliente_table.csv.gz'\n",
    "OUTPUT_DIR = 'submission'\n",
    "NUM_CPUS = 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "def skip_header(x):\n",
    "    if not re.search(r'[A-Za-z]',x):\n",
    "        return x\n",
    "\n",
    "train_raw = sc.textFile(DATA_DIR+TRAIN_FILE).filter(skip_header).repartition(NUM_CPUS*2)\n",
    "train, val = train_raw.randomSplit([0.8,0.2])\n",
    "train.cache()\n",
    "val.cache()\n",
    "\n",
    "test = sc.textFile(DATA_DIR+TEST_FILE).filter(skip_header).repartition(NUM_CPUS*2).cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### create OHE dict from training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parsedTrainData = train.map(\n",
    "    lambda x: parsePoint(x, 1, -5)\n",
    ")\n",
    "\n",
    "oheDict = createOneHotDict(parsedTrainData)\n",
    "numOHEFeats = len(oheDict.keys())\n",
    "\n",
    "oheTrainData = train.map(lambda point: parseOHEPoint(point, oheDict, numOHEFeats))\n",
    "oheValData = val.map(lambda point: parseOHEPoint(point, oheDict, numOHEFeats))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge Linear Regression model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "step_sizes = [ 1, 0.1, 1e-2 ]\n",
    "regParams = [ 10, 1, 0.1, 1e-2 ]\n",
    "results = []\n",
    "\n",
    "for s in step_sizes:\n",
    "    for r in regParams:\n",
    "        lr_model = LinearRegressionWithSGD.train(\n",
    "            data=oheTrainData ,\n",
    "            iterations=30 ,\n",
    "            step=s ,\n",
    "            regType='l2' ,\n",
    "            regParam=r ,\n",
    "            convergenceTol=1e-3 ,\n",
    "            intercept=True ,\n",
    "            initialWeights=np.ones(numOHEFeats)\n",
    "        )\n",
    "        preds = oheValData.map(\n",
    "            lambda x: predict(x , \n",
    "                              lr_model.weights , \n",
    "                              lr_model.intercept)\n",
    "        )\n",
    "        rm = rmsle(preds)\n",
    "        results.append({'step_size':s,'regParam':r,'result':rm})\n",
    "\n",
    "print results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s = 0.1  # from grid search\n",
    "r = 1    # from grid search\n",
    "lr_model = LinearRegressionWithSGD.train(\n",
    "    data=oheTrainData ,\n",
    "    iterations=600 ,\n",
    "    step=s ,\n",
    "    regType='l2' ,\n",
    "    regParam=r ,\n",
    "    convergenceTol=1e-3 ,\n",
    "    intercept=True ,\n",
    "    initialWeights=np.ones(numOHEFeats)\n",
    ")\n",
    "preds = oheValData.map(\n",
    "    lambda x: predict(x , \n",
    "                      lr_model.weights , \n",
    "                      lr_model.intercept)\n",
    ")\n",
    "rm = rmsle(preds)\n",
    "print 'Model RMSLE: {}'.format(rm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### predict test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "oheTestData = test.map(lambda point: parseOHEPoint(point, \n",
    "                                                   oheDict, \n",
    "                                                   numOHEFeats,\n",
    "                                                   2,\n",
    "                                                   None,\n",
    "                                                   train=False))\n",
    "\n",
    "test_preds = oheTestData.map(\n",
    "    lambda x: predict(x ,\n",
    "                      lr_model.weights ,\n",
    "                      lr_model.interept)\n",
    ").cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### save results to output directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_preds.map(lambda x: x[0]).zipWithIndex()\\\n",
    "            .map(lambda x: '{},{}'.format(x[1],x[0]))\\\n",
    "            .saveAsTextFile(DATA_DIR+OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Random Forest model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.tree import RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rf_model = RandomForest.trainRegressor(\n",
    "    data=oheTrainData,\n",
    "    categoricalFeaturesInfo={},\n",
    "    numTrees=10,\n",
    "    featureSubsetStrategy='onethird',\n",
    "    maxDepth=6,\n",
    "    seed=22,\n",
    ")\n",
    "preds = oheValData.map(\n",
    "    lambda x: rf_model.predict(x)\n",
    ")\n",
    "rm = rmsle(preds)\n",
    "print 'Model RMSLE: {}'.format(rm)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
