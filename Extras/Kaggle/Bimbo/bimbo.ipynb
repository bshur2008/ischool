{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Kaggle -- Bimbo\n",
    "Demand prediction for Mexican food company Bimbo stores+clients+products. Will use Spark because training data is somewhat sizable (>3GB uncompressed) and there are quite a lot of combinations for the different Agencies, Channels, Routes, Clients, and Products. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys #current as of 9/26/2015\n",
    "spark_home = os.environ['SPARK_HOME'] = \\\n",
    "   '/opt/spark16'\n",
    "    \n",
    "sys.path.insert(0,os.path.join(spark_home,'python'))\n",
    "sys.path.insert(0,os.path.join(spark_home,'python/lib/py4j-0.8.2.1-src.zip'))\n",
    "execfile(os.path.join(spark_home,'python/pyspark/shell.py'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load functions and args"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### load functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import hashlib\n",
    "import numpy as np\n",
    "from math import log, sqrt\n",
    "from collections import defaultdict\n",
    "from pyspark.mllib.linalg import SparseVector\n",
    "from pyspark.mllib.regression import LabeledPoint, LinearRegressionWithSGD\n",
    "\n",
    "\n",
    "def hashFunction(numBuckets, rawFeats, printMapping=False):\n",
    "    \"\"\"Calculate a feature dictionary for an observation's features based on hashing.\n",
    "\n",
    "    Note:\n",
    "        Use printMapping=True for debug purposes and to better understand how the hashing works.\n",
    "\n",
    "    Args:\n",
    "        numBuckets (int): Number of buckets to use as features.\n",
    "        rawFeats (list of (int, str)): A list of features for an observation.  Represented as\n",
    "            (featureID, value) tuples.\n",
    "        printMapping (bool, optional): If true, the mappings of featureString to index will be\n",
    "            printed.\n",
    "\n",
    "    Returns:\n",
    "        dict of int to float:  The keys will be integers which represent the buckets that the\n",
    "            features have been hashed to.  The value for a given key will contain the count of the\n",
    "            (featureID, value) tuples that have hashed to that key.\n",
    "    \"\"\"\n",
    "    mapping = {}\n",
    "    for ind, category in rawFeats:\n",
    "        featureString = category + str(ind)\n",
    "        mapping[featureString] = int(int(hashlib.md5(featureString).hexdigest(), 16) % numBuckets)\n",
    "    if(printMapping): print mapping\n",
    "    sparseFeatures = defaultdict(float)\n",
    "    for bucket in mapping.values():\n",
    "        sparseFeatures[bucket] += 1.0\n",
    "    return dict(sparseFeatures)\n",
    "\n",
    "\n",
    "def parseHashPoint(point, numBuckets, start_i=1, end_i=-5, sep=',', train=True):\n",
    "    \"\"\"Create a LabeledPoint for this observation using hashing.\n",
    "\n",
    "    Args:\n",
    "        point (str): A comma separated string where the first value is the label and the rest are\n",
    "            features.\n",
    "        numBuckets: The number of buckets to hash to.\n",
    "        sep: The separator of the input features.\n",
    "\n",
    "    Returns:\n",
    "        LabeledPoint: A LabeledPoint with a label (0.0 or 1.0) and a SparseVector of hashed\n",
    "            features.\n",
    "    \"\"\"\n",
    "    data = point.split(sep)\n",
    "    features = data[start_i:end_i]\n",
    "    indexed_features = [(i, feature) for i, feature in enumerate(features)]\n",
    "    if train:\n",
    "        label = data[-1]\n",
    "        return LabeledPoint(label, SparseVector(numBuckets, hashFunction(numBuckets, indexed_features)))\n",
    "    else:\n",
    "        return SparseVector(numBuckets, hashFunction(numBuckets, indexed_features))\n",
    "\n",
    "\n",
    "def predict(x, weights, intercept):\n",
    "    ''' Make a prediction \n",
    "        from x \n",
    "        based on model weights\n",
    "    '''\n",
    "    prediction = x.dot(weights) + intercept\n",
    "    return (prediction, x)\n",
    "\n",
    "\n",
    "def rmsle(preds):\n",
    "    ''' With predictions\n",
    "        output RMSLE\n",
    "    '''\n",
    "    N = preds.count()\n",
    "    pred_log_avg_sq_error = preds.map(\n",
    "        lambda (x,p): (log(p+1)-log(x+1))**2\n",
    "    ).reduce( \n",
    "        lambda x,y: x+y\n",
    "    ).collect() / N\n",
    "    return sqrt(pred_log_avg_sq_error)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### declare args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DATA_DIR = 'Data/'\n",
    "TRAIN_FILE = 'train.csv.gz'\n",
    "TEST_FILE = 'test.csv.gz'\n",
    "PRODUCT_FILE = 'producto_tabla.csv.gz'\n",
    "CLIENT_FILE = 'cliente_table.csv.gz'\n",
    "OUTPUT_DIR = 'submission'\n",
    "NUM_CPUS = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "def skip_header(x):\n",
    "    if not re.search(r'[A-Za-z]',x):\n",
    "        return x\n",
    "\n",
    "train_raw = sc.textFile(DATA_DIR+TRAIN_FILE).filter(skip_header).repartition(NUM_CPUS*2)\n",
    "train, val = train_raw.randomSplit([0.8,0.2])\n",
    "train.cache()\n",
    "val.cache()\n",
    "\n",
    "test = sc.textFile(DATA_DIR+TEST_FILE).filter(skip_header).repartition(NUM_CPUS*2).cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### create hashed training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hashTrainData = train.map(\n",
    "    lambda x: parseHashPoint(x, 2**15)\n",
    ").cache()\n",
    "\n",
    "hashValData = val.map(\n",
    "    lambda x: parseHashPoint(x, 2**15, train=False)\n",
    ").cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lasso Linear Regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lr_model = LinearRegressionWithSGD.train(\n",
    "    data=hashTrainData ,\n",
    "    iterations=30 ,\n",
    "    step=10.0 ,\n",
    "    miniBatchFraction=0.75 ,\n",
    "    regType='l2' ,\n",
    "    regParam=0.001 ,\n",
    "    convergenceTol=1e-5 ,\n",
    "    intercept=True ,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "preds = hashValData.map(\n",
    "    lambda x: predict(x , \n",
    "                      lr_model.weights , \n",
    "                      lr_model.intercept)\n",
    ")\n",
    "rm = rmsle(preds)\n",
    "print 'Val RMSLE: {}'.format(rmsle(preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### predict test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hashTestData = test.map(lambda x: parseHashPoint(x, \n",
    "                                                 2**15, \n",
    "                                                 2, \n",
    "                                                 None, \n",
    "                                                 train=False))\n",
    "\n",
    "test_preds = hashTestData.map(\n",
    "    lambda x: predict(x ,\n",
    "                      lr_model.weights ,\n",
    "                      lr_model.interept)\n",
    ").cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### save results to output directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_preds.map(lambda x: x[0]).zipWithIndex()\\\n",
    "            .map(lambda x: '{},{}'.format(x[1],x[0]))\\\n",
    "            .saveAsTextFile(DATA_DIR+OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
