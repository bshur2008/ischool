{
 "metadata": {
  "name": "Bag of Words p2 (Kaggle)"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "[Part 2: Word Vectors](https://www.kaggle.com/c/word2vec-nlp-tutorial/details/part-2-word-vectors)\n",
      "=================\n",
      "\n",
      "Description:\n",
      "----------\n",
      "\n",
      "This tutorial uses distributed word vectors created by the [Word2Vec algorithm](https://code.google.com/p/word2vec/), via Google.\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pandas as pd\n",
      "\n",
      "# Read data from files \n",
      "train = pd.read_csv( \"Data/labeledTrainData.tsv\", header=0, \n",
      "    delimiter=\"\\t\", quoting=3 )\n",
      "test = pd.read_csv( \"Data/testData.tsv\", header=0, \n",
      "    delimiter=\"\\t\", quoting=3 )\n",
      "unlabeled_train = pd.read_csv( \"Data/unlabeledTrainData.tsv\", header=0, \n",
      "    delimiter=\"\\t\", quoting=3 )\n",
      "\n",
      "# Verify the number of reviews that were read (100,000 in total)\n",
      "print \"Read %d labeled train reviews, %d labeled test reviews, \" \\\n",
      " \"and %d unlabeled reviews\\n\" % (train[\"review\"].size,  \n",
      " test[\"review\"].size, unlabeled_train[\"review\"].size )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Read 25000 labeled train reviews, 25000 labeled test reviews, and 50000 unlabeled reviews\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The functions we write to clean the data are also similar to Part 1, although now there are a couple of differences. First, to train Word2Vec it is better not to remove stop words because the algorithm relies on the broader context of the sentence in order to produce high-quality word vectors. For this reason, we will make stop word removal optional in the functions below. It also might be better not to remove numbers, but we leave that as an exercise for the reader."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Import various modules for string cleaning\n",
      "from bs4 import BeautifulSoup\n",
      "import re\n",
      "from nltk.corpus import stopwords\n",
      "letters_only = lambda x: re.sub(\"[^a-zA-Z]\",\" \", x)\n",
      "\n",
      "def review_to_wordlist( review, remove_stopwords=False ):\n",
      "    ''' Function to convert a document to a sequence of words,\n",
      "        optionally removing stop words.  Returns a list of words.\n",
      "    '''\n",
      "    review_text = BeautifulSoup(review).get_text()\n",
      "    review_text = letters_only(review_text)\n",
      "    words = review_text.lower().split()\n",
      "    if remove_stopwords:\n",
      "        stops = set(stopwords.words(\"english\"))\n",
      "        words = [w for w in words if not w in stops]\n",
      "    return(words)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We need lists of full sentences, and because English is complicated, we'll use NLTK's punkt tokenizer for sentence splitting. In order to use this, you will need to install NLTK and use nltk.download() to download the relevant training file for punkt."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Load the punkt tokenizer\n",
      "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Define a function to split a review into parsed sentences\n",
      "def review_to_sentences( review, tokenizer, remove_stopwords=False ):\n",
      "    ''' Function to split a review into parsed sentences. Returns a \n",
      "        list of sentences, where each sentence is a list of words\n",
      "    '''\n",
      "    # 1. Use the NLTK tokenizer to split the paragraph into sentences\n",
      "    raw_sentences = tokenizer.tokenize(review.strip())\n",
      "    #\n",
      "    # 2. Loop over each sentence\n",
      "    sentences = []\n",
      "    for raw_sentence in raw_sentences:\n",
      "        # If a sentence is empty, skip it\n",
      "        if len(raw_sentence) > 0:\n",
      "            # Otherwise, call review_to_wordlist to get a list of words\n",
      "            sentences.append( review_to_wordlist( raw_sentence, \\\n",
      "              remove_stopwords ))\n",
      "    #\n",
      "    # Return the list of sentences (each sentence is a list of words,\n",
      "    # so this returns a list of lists\n",
      "    return sentences\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Apply the function to prepare our data for input to Word2Vec (this will take a couple minutes):"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sentences = []  # Initialize an empty list of sentences\n",
      "\n",
      "print \"Parsing sentences from training set\"\n",
      "for review in train[\"review\"]:\n",
      "    sentences += review_to_sentences(review, tokenizer)\n",
      "\n",
      "print \"Parsing sentences from unlabeled set\"\n",
      "for review in unlabeled_train[\"review\"]:\n",
      "    sentences += review_to_sentences(review, tokenizer)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "/Library/Python/2.7/site-packages/bs4/__init__.py:182: UserWarning: \".\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
        "  '\"%s\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.' % markup)\n",
        "/Library/Python/2.7/site-packages/bs4/__init__.py:189: UserWarning: \"http://www.happierabroad.com\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.\n",
        "  '\"%s\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.' % markup)\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Parsing sentences from training set\n",
        "Parsing sentences from unlabeled set"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "/Library/Python/2.7/site-packages/bs4/__init__.py:189: UserWarning: \"http://www.archive.org/details/LovefromaStranger\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.\n",
        "  '\"%s\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.' % markup)\n",
        "/Library/Python/2.7/site-packages/bs4/__init__.py:189: UserWarning: \"http://www.loosechangeguide.com/LooseChangeGuide.html\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.\n",
        "  '\"%s\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.' % markup)\n"
       ]
      }
     ],
     "prompt_number": "*"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Import the built-in logging module and configure it so that Word2Vec \n",
      "# creates nice output messages\n",
      "from gensim.models import word2vec\n",
      "import logging\n",
      "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',\\\n",
      "    level=logging.INFO)\n",
      "\n",
      "# Set values for various parameters\n",
      "num_features = 300    # Word vector dimensionality                      \n",
      "min_word_count = 40   # Minimum word count                        \n",
      "num_workers = 4       # Number of threads to run in parallel\n",
      "context = 10          # Context window size                                                                                    \n",
      "downsampling = 1e-3   # Downsample setting for frequent words\n",
      "\n",
      "# Initialize and train the model (this will take some time)\n",
      "print \"Training model...\"\n",
      "model = word2vec.Word2Vec(sentences, workers=num_workers, \\\n",
      "            size=num_features, min_count = min_word_count, \\\n",
      "            window = context, sample = downsampling)\n",
      "\n",
      "# If you don't plan to train the model any further, calling \n",
      "# init_sims will make the model much more memory-efficient.\n",
      "model.init_sims(replace=True)\n",
      "\n",
      "# It can be helpful to create a meaningful model name and \n",
      "# save the model for later use. You can load it later using Word2Vec.load()\n",
      "model_name = \"300features_40minwords_10context\"\n",
      "model.save(model_name)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}